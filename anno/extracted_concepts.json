{"indexed_table_303.csv": {"Caption": "Table 1: Accuracies Obtained by the Different Methods on ICD-9 Data", "Rows_1": ["simSVM", "1vsR SVM", "rankSVM", "BPMLL", "MLKNN"], "Column_2": ["monia", "collapse", "fever", "adhesion", "bladder", "disorder", "infect", "turia"], "Column_1": ["Pneu", "Asthma", "Pulm", "Rheu", "Kidney", "Neuro", "Bladder", "Urinary", "Hema"]}, "indexed_table_404.csv": {"Caption": "Table 7: The win/tie/loss results (grouped by |S|) for LEAD against the compared algorithms in terms of different evaluation metrics.", "Rows_1": ["hamming loss", "one error", "coverage", "ranking loss", "average precision", "In Total"], "Column_2": ["|S| < 5000", "|S| > 500", "|S| < 500"], "Column_1": ["Bsvm", "Ml knn", "Bp mll", "Ecc"]}, "indexed_table_28.csv": {"Caption": "Table 4: The average access time per edge for processing adjacency queries and (in+out) neighbor queries.", "Rows_1": ["amazon", "amazon0312", "ca CondMat", "ca HepPh", "cit HepPh", "cit Patents", "email Enron", "email EuAll", "p2p Gnutella08", "p2p Gnutella", "soc LiveJournal1", "soc Slashdot", "web Google"], "Column_2": ["SNAP"], "Column_1": ["adj queries", "Neigh queries"]}, "indexed_table_156.csv": {"Caption": "Table 3: Comparison of Precisions on Synthetic Data", "Rows_1": ["M = 1000 r = 1%", "M = 1000 r = 5%", "M = 2000 r = 1%", "M = 2000 r = 5%", "M = 5000 r = 1%", "M = 5000 r = 5%"], "Column_2": ["GLODA", "DNODA", "CNA", "CODA"], "Column_1": ["K = 5", "K = 8"]}, "indexed_table_21.csv": {"Caption": "Table 2: LETOR 4.0 Results. The regression-only method performs best on Mean Squared Error (MSE), while the ranking-only method performs best on the rank-based metrics Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG). The combined CRR method performs nearly as well as the ranking-only method on MAP and NDCG, with MSE approaching that of the regression-only method. The baseline results for RankSVM are previously reported benchmark results [18].", "Rows_1": ["Regression", "RankSVM", "Ranking", "CRR"], "Column_2": ["MAP", "NDCG", "MSE"], "Column_1": ["MQ2007", "MQ2008"]}, "indexed_table_118.csv": {"Caption": "Table 3. Precision, Recall, and F-measure on Simulation Data.", "Rows_1": ["HMM", "kNN GMM"], "Column_2": ["Mean"], "Column_1": ["Toilet", "Shower", "Washer"]}, "indexed_table_97.csv": {"Caption": "Table 2: Example tagged items dataset", "Rows_1": ["ID", "1", "2", "3", "4", "5", "6", "7", "8"], "Column_2": ["A1", "A2", "A3", "A4", "T1"], "Column_1": ["Attribute", "Tag"]}, "indexed_table_121.csv": {"Caption": "Table 4: Averaged Results for D1 and D2 when predicting users\u2019 travel profiles: How many trips per day will they take? What zones will they travel between? Will they only travel off-peak? Lastly, will they only take bus trips?", "Rows_1": ["Last Profile", "Avg Profile", "Moving Avg"], "Column_2": ["Span", "MAE", "Precision", "Recall"], "Column_1": ["Avg Trips Day", "Geography", "Travel Time", "Travel Modality"]}, "indexed_table_268.csv": {"Caption": "Table 1: Percentages of vertices that violate k2- degree anonymity.", "Rows_1": ["k", "5", "10", "15", "20"], "Column_2": ["Degree", "Friendship"], "Column_1": ["Original G", "k degree G", "k2 degree G"]}, "indexed_table_150.csv": {"Caption": "Table 2: Object clustering performance of different methods on (a) DBLP and (b) NSF-Awards datasets.", "Rows_1": ["NMF", "PLSA", "LapPLSI", "LDA", "ATM", "NetClus", "TMBP RW", "TMBP Regu"], "Column_2": ["AC", "NMI"], "Column_1": ["Paper", "Author"]}, "indexed_table_368.csv": {"Caption": "Table 6: The numbers that the top ten cited authors (papers) are cited by the other authors (papers) of different rankings in Experiments 3(a).", "Rows_1": ["ranking"], "Column_2": ["ranking"], "Column_1": ["Experiment 3 author", "Experiment 3 paper"]}, "indexed_table_360.csv": {"Caption": "Table 1: Overall classification results for the political affiliation level wrt B1 alone, B2 alone, and both B1 and B2.", "Rows_1": ["B1", "B2", "BEHAV ALL", "PROF ALL", "SOC REPL", "SOC RETW", "SOC FRIE", "SOC ALL", "LING HASH", "LING WORD", "LING GLDA", "LING DLDA", "LING SENT", "LING ALL", "ML", "GRAPH", "HYBRID"], "Column_2": ["PREC", "REC", "F MEASURE", "ACC"], "Column_1": ["Democrats", "Republicans", "All"]}, "indexed_table_153.csv": {"Caption": "Table 4: Comparisons among benchmark methods", "Rows_1": ["Syn 5", "Syn 10", "Intrusion", "Spam", "URL", "Adult", "Magic", "Winered", "Winewhite", "Census"], "Column_2": ["Time", "Memory", "Error"], "Column_1": ["LE tree", "L Ensemble", "GE tree", "G Ensemble"]}, "indexed_table_57.csv": {"Caption": "Table 2: Average prediction error & stability scores of the single model aggregation operators over all datasets when SVM is used as the classification algorithm. In each cell the upper value is the average error score of the model mining operator given in the row, over the different model mining operators, for the feature selection algorithm specified in the column header; the lower value is the respective stability score. The final column gives the respective averages for each model mining operator over the different feature selection algorithms. The larger the values the higher the rank of the method; the top rank per column is indicated in bold.", "Rows_1": ["no bagg", "avg Rank", "most Freq", "max Apr", "closed Apr", "med Max Apr", "med Closed Apr", "large Med", "most Rep"], "Column_2": ["IG", "CHI", "SYM", "RELIEF", "SVMONE", "SVMRFE", "CFS"], "Column_1": ["Univariate Feature Selection", "Multivariate Feature Selection"]}, "indexed_table_40.csv": {"Caption": "Table 2: Comparisons between SVT and ASVT on the synthetic dataset with different settings (matrix size (m,n), rank r, observed ratio p and parameter \u03c4). As can be seen, our proposed ASVT can accelerate the convergence by 2-7 orders of magnitude.", "Rows_1": [], "Column_2": ["SVT", "ASVT"], "Column_1": ["Settings", "iteration", "error after 50 iterations"]}, "indexed_table_127.csv": {"Caption": "Table 2: Classification performance comparison of the proposed iMSF, ScoreComp and missing value estimation methods (Zero, EM, KNN, SVD and SVT) in terms of accuracy, sensitivity and specificity when the training percentage varies from 50% to 75%.", "Rows_1": ["iMSF Best", "iMSF Average", "ScoreComp Zero", "ScoreComp EM", "ScoreComp KNN", "Zero", "EM", "KNN", "SVD", "SVT", "Baseline"], "Column_2": ["Accuracy", "Sensitivity", "Specificity"], "Column_1": ["AD vs Normal", "AD vs Non converter", "Converter vs Normal"]}, "indexed_table_294.csv": {"Caption": "Table 4: Classification comparison of using multi-modality data (iMSF and ScoreComp) and using the complete MRI + CSF + PET + Proteomics data. Classification is performed on the same set of samples, so that a fair comparison can be made. Leave-one-out is used and the accuracy, sensitivity and specificity are reported.", "Rows_1": ["Baseline", "iMSF Average", "ScoreComp EM"], "Column_2": ["Accuracy", "Sensitivity", "Specificity"], "Column_1": ["AD vs Normal", "AD vs Non converter", "Converter vs Normal"]}, "indexed_table_354.csv": {"Caption": "Table 4: Link Prediction Performances.", "Rows_1": ["PA", "Katz", "GS", "SCMiner"], "Column_2": ["Precision", "Recall"], "Column_1": ["BP1", "BP2", "BP3", "Jester"]}, "indexed_table_331.csv": {"Caption": "Table 1: Classification performance comparison of different algorithms on the DBLP dataset. Except for the results of the FTM and cFTM, all the other results were reported in [6].", "Rows_1": ["NMF", "PLSA", "LapPLSI", "LDA", "ATM", "NetClus", "TMBP RW", "TMBP Regu", "FTM", "cFTM"], "Column_2": ["AC", "NMI"], "Column_1": ["Paper", "Author", "Venue", "Average"]}, "indexed_table_314.csv": {"Caption": "Table 1: Basic statistics and results per dataset", "Rows_1": ["Indep", "Plants 10", "Plants 50", "Addresses", "JMLR", "Moby"], "Column_2": ["|D|", "L", "|F|", "|G|", "L"], "Column_1": ["SQS CANDIDATES", "SQS SEARCH"]}, "indexed_table_52.csv": {"Caption": "Table3: The best community detected by the six methods explored. The first (\u00a76) and last (\u00a77) are ours.", "Rows_1": ["ca AstroPh", "email Enron", "cond mat 2005", "arxiv", "dblp", "hollywood 2009", "Penn94", "fb A oneyear", "fb A", "soc LiveJournal1", "oregon2 010526", "p2p Gnutella25", "as 22july06", "itdk0304", "web Google"], "Column_2": ["Cond", "Size"], "Column_1": ["Neighborhood", "Fiedler", "PageRank", "Whisker", "Metis", "Seeded"]}, "indexed_table_197.csv": {"Caption": "Table 4: Evaluation results", "Rows_1": ["Roy Halladay", "CC Sabathia", "Jon Lester", "Average on 76 pitchers"], "Column_2": ["Ours", "Manager"], "Column_1": ["Accuracy", "F Score", "Odds Ratio"]}, "indexed_table_269.csv": {"Caption": "Table 6: Size of TG on ST data set", "Rows_1": [], "Column_2": [], "Column_1": ["size", "max dist err(m"]}, "indexed_table_432.csv": {"Caption": "Table 3: Root Mean Square Error (RMSE), Error Reduction (ER, in %) and Error Standard Deviation Reduction(SDR, in %) of TiSe and TiSe-Q compared to SVM. Both TiSe and TiSe-Q achieved significant SDR reduction, though RMSE increased in several cases for TiSe. In bold is the method with the lowest RMSE.", "Rows_1": ["Imports", "Starbucks", "Amazon com", "British Airways", "Apple", "Fedex", "Johnson & Johnson", "Chocolate", "Ford", "IBM", "Boeing", "Auto registrations", "Earth rotations", "Radioactivity", "Simulated sample", "Island Pacific", "McDonalds", "Simulated sample", "Water usage", "American Express", "Microsoft", "Walt Disney", "AUD/USD exch", "Hewlett Packard", "Colgate Plamolive", "Earthquakes", "Tree", "Intel", "Coke", "Temp anomalies", "Siemens", "Ebay", "Employment", "Rhine", "Chemical process", "Robberies", "Simulated sample", "Airline passengers", "Simulated sample"], "Column_2": ["ARIMA", "NN", "KNN", "SVM", "RBF", "RR", "TiSe", "TiSe Q"], "Column_1": ["RMSE", "ER", "SDR"]}, "indexed_table_123.csv": {"Caption": "Table 1: Details of Artificial Data Set", "Rows_1": ["Dataset 1", "Dataset 2", "Dataset 3", "Dataset 4"], "Column_2": ["ave", "max", "min"], "Column_1": ["frequency", "period", "period", "customers"]}, "indexed_table_136.csv": {"Caption": "Table 2: Artificial Data Results", "Rows_1": ["Conventional", "K given model", "K given model", "K given model", "Proposed", "K given model", "K given model", "K given model"], "Column_2": [], "Column_1": ["MSE log", "MSE log", "MSE coef for", "Correlation coef for", "Correlation log", "Correlation log", "loglikehood", "log likehood"]}, "indexed_table_257.csv": {"Caption": "Table 4: The training time (in mins) for all the methods.", "Rows_1": ["CLEF", "RCV1", "IPC", "LSHTC small", "DMOZ 2010", "DMOZ 2012", "DMOZ 2011", "SWIKI", "LWIKI"], "Column_2": ["SVM", "HR SVM", "LR", "HR LR", "TD", "HSVM", "OT", "HB LR"], "Column_1": ["Comparison with Flat baselines", "Other Hierarchical Baselines"]}, "indexed_table_363.csv": {"Caption": "Table1: The runtime and memory requirements of the social vector clocks calculations depends heavily on the reach parameter \u03bc. Memory (in MB) indicates the peak amount of memory needed by our", "Rows_1": ["irvine", "election", "studivz", "mobile com"], "Column_2": ["N Event", "N Node", "Mem", "Time"], "Column_1": []}, "indexed_table_389.csv": {"Caption": "Table 2: Link prediction statistics by dataset and path length N . These values report the average over all realizations of each experiment. For each realization, measurements are based on the graph associated with the interval [t\u20320 , t\u20321 ) and the new edges formed in the interval [t\u20321 , t\u20322 ).", "Rows_1": ["election", "election", "olympics", "irvine", "studivz"], "Column_2": ["Realizations", "Avg Pos", "Avg Neg"], "Column_1": ["N = 2", "N = 3", "N = 4"]}, "indexed_table_37.csv": {"Caption": "Table 2: Performance of Coherence Measures.", "Rows_1": ["ptpn", "PTPptpbe", "ptpe", "ptpm", "scoas", "SCOscobs", "SIM all variants", "PTP+SCO", "SD SDlsi", "RL all variants", "PTP+SD", "SCO+SD", "PTP+SCO+SD"], "Column_2": ["ER", "TPR", "FPR"], "Column_1": ["linear SVM", "polynomial SVM", "NB"]}, "indexed_table_355.csv": {"Caption": "Table 4. Verification results of candidate-to-candidate functions (in percentage), Pre. = precision, Rec. = recall.", "Rows_1": ["Function", "OI", "FI", "OF", "CC", "All"], "Column_2": ["Pre", "Rec"], "Column_1": ["Foursquare", "Twitter", "Plurk", "DBLP"]}, "indexed_table_231.csv": {"Caption": "Table 1: Performance in terms of AUC of baseline keyword matching approach, text classifiers and spell- correction. Statistically significant improvements over Keywords (Column 2) are presented in bold.", "Rows_1": ["education", "emergency", "employment", "energy", "family", "health", "orphans", "social", "report", "violence", "water", "Mean"], "Column_2": ["Keywords", "Naive Bayes", "SVM"], "Column_1": ["Original Messages", "Spell Corrected"]}, "indexed_table_419.csv": {"Caption": "Table 1: Comparison of different algorithms on single sub-graphs.", "Rows_1": ["LS", "LS Data", "LS Net", "AdaBoost", "SS Margin Boost", "MultiTask Boost", "Social Boost"], "Column_2": ["Entertainment", "Living", "Outdoor"], "Column_1": ["Sub Graph One", "Sub Graph Two", "Sub Graph Three"]}, "indexed_table_125.csv": {"Caption": "Table 2: User study results, for 3 level and 4 level hierarchies. Higher values indicate a higher quality constructed hierarchy", "Rows_1": ["Levels", "hPAM", "hPAMrr", "SpecClus", "CATHY", "CATHY", "Levels", "CATHYcp"], "Column_2": ["Correct", "Answered"], "Column_1": ["Topic Intrusion", "Phrase Intrusion"]}, "indexed_table_417.csv": {"Caption": "Table 3: Comparing the forecasting accuracy of different models in EMBERS. Quality scores in this and other tables are in the range [0,4] where 4 is the most accurate. AR=Argentina; BR=Brazil; CL=Chile; CO=Colombia; EC=Ecuador; SV=El Salvador; MX=Mexico; PY=Paraguay; UY=Uruguay; VE=Venezuela. A _ indicates that the model did not produce any warnings for that country in the studied period.", "Rows_1": ["Dynamic Query Expansion", "Volume based Model", "MLE", "Planned Protest", "Cascades Model"], "Column_1": ["AR", "BR", "CL", "CO", "EC", "SV", "MX", "PY", "UY", "VE", "All"]}, "indexed_table_292.csv": {"Caption": "Table 4: EMBERS metrics across multiple countries.", "Rows_1": ["Quality score", "Recall", "Precision", "Lead time", "Probability measure"], "Column_1": ["AR", "BR", "CL", "CO", "EC", "MX", "PY", "SV", "UY", "VE", "All"]}, "indexed_table_209.csv": {"Caption": "Table 2: Accuracy (mean\u00b1std.) comparison on regular scale data sets. Linear kernels are used. The best accuracy on each data set is bolded. _/_ indicates the performance is significantly better/worse than SVM (paired t-tests at 95% significance level). The win/tie/loss counts are summarized in the last row.", "Rows_1": ["promoters", "planning relax", "colic", "parkinsons", "colic ORIG", "sonar", "vote", "house", "heart", "breast cancer", "haberman", "vehicle", "clean1", "wdbc", "isolet", "credit", "austra", "australian", "fourclass"], "Column_1": ["SVM", "MDO", "MAMC", "KM OMD", "LDM"]}, "indexed_table_202.csv": {"Caption": "Table 3: Accuracy (mean\u00b1std.) comparison on regular scale data sets. RBF kernels are used. The best accuracy on each data set is bolded. _/_ indicates the performance is significantly better/worse than SVM (paired t-tests at 95% significance level). The win/tie/loss counts are summarized in the last row. MDO does not have results since it is specified for the linear kernel.", "Rows_1": ["farm ads", "news", "adult", "w8a", "cod rna", "real sim", "ijcnn", "skin", "covtype", "rcv", "url", "kdd"], "Column_1": ["SVM", "MDO", "MAMC", "KM OMD", "LDM"]}, "indexed_table_12.csv": {"Caption": "Table 1: Dataset Information, Parameter Search Results and Running Time for The Best Shapelet [8]", "Rows_1": ["Adiac", "Beef", "Beetle Fly", "Bird Chicken", "Chlorine", "Coffee", "Diatom", "DP Little", "DP Middle", "DP Thumb", "ECG Five Days", "Face Four", "Gun Point", "Italy Power", "Lighting", "Medical Images", "Mote Strain", "MP Little", "MP Middle", "Otoliths", "PP Little", "PP Middle", "PP Thumb", "Sony AIBO", "Symbols", "Synthetic Control", "Trace", "Two Lead ECG"], "Column_2": ["Length", "Cls", "minK", "L", "R", "maxIter", "F Stat", "LTS"], "Column_1": ["Dataset Information", "Parameter Values", "Run Time"]}, "indexed_table_184.csv": {"Caption": "Table 4: Improvement of Max Permanence with respect to the average (left-hand value) and the best (right- hand value) performances of the six competing algorithms (separated by semicolon). Positive (negative) values indicate that Max Permanence outperforms (underperforms) the corresponding performances of the competing algorithms.", "Rows_1": ["NMI", "ARI", "PU", "W NMI", "W ARI", "W PU"], "Column_1": ["LFR", "LFR", "LFR", "Football", "Railway", "Coauthorship"]}, "indexed_table_239.csv": {"Caption": "Table 3: Classification error rates on the benchmark datasets (error rates are in %). Models: SVM-RBF: SVM with RBF kernels. SVM-Poly: SVM with polynomial kernels. NNet: (MLP) Feed-forward results from [18]. The best results obtained by all these models are marked in bold.  neural net. GSM: Gated softmax classifier. NonGSM: Non- factored gated softmax classifier. SAA-3: Three-layer stacked auto-associator. RBM: Restricted boltzmann machine. SUGAR-3: Three-layer stacked SUGAR. The results of SVM-RBF, SVM-Poly, NNet, SAA-3 and RBM are taken from [15], GSM and NonGSM", "Rows_1": ["Rectangles", "RectImg", "Convex", "MNISTBasic", "MNISTRot", "MNISTRand", "MNISTImg", "MNISTRotImg", "Average"], "Column_1": ["SVM RBF", "SVM Poly", "NNet", "GSM", "NonGSM", "SAA", "RBM", "SUGAR"]}, "indexed_table_343.csv": {"Caption": "Table 1: F1 scores achieved and the number of classes recovered by each of the four techniques on the artificial data set.", "Rows_1": ["DPM", "HDPM", "HDPM RE", "ASPIRE"], "Column_1": ["F1 Scores"]}, "indexed_table_27.csv": {"Caption": "Table 3: Flexible FEMA has the smallest MAE and RMSE on 3W Prediction tasks when t = 10 and \u03b1t = 80%. The model is better if the MAE and RMSE are smaller.", "Rows_1": ["FEMA", "EMA", "DTA", "HOSVD", "CP"], "Column_2": ["MAE", "RMSE"], "Column_1": ["MAS", "WEIBO"]}, "indexed_table_275.csv": {"Caption": "Table 1: Prediction AUC Comparison for Different Methods and Different Tasks", "Rows_1": ["NN", "SVM", "LR", "SLR", "MLR", "MulSLR"], "Column_1": ["normal vs MCI and demented", "MCI vs normal and demented", "demented vs normal and MCI"]}, "indexed_table_9.csv": {"Caption": "Table 5: Recommendation performance comparisons on DBLP and PubMed datasets in terms of Precision, Recall and MRR. We set the number of interest groups to be 200 (K = 200) for ClusCite and ClusCite-Rel.", "Rows_1": ["ClusCite", "BM25", "PopRank", "TopicSim", "Link PLSA LDA", "L2 LR", "RankSVM", "MixFea", "ClusCite Rel"], "Column_2": ["P@10", "P@20", "R@20", "R@50", "MRR"], "Column_1": ["DBLP", "PubMed"]}, "indexed_table_115.csv": {"Caption": "Table 3: Clustering Accuracy on Real Datasets", "Rows_1": ["Umist", "Coil20", "Jaffe50", "USPS", "Palm", "MSRA50", "Stock", "Pathbased", "Movements", "Spiral", "Wine", "Compound", "Yeast", "Glass", "Ecoli"], "Column_1": ["K-Means", "Ratio Cut", "Normalized Cut", "NMF", "CAN", "PCAN"]}, "indexed_table_245.csv": {"Caption": "Table 4: Clustering NMI on Real Datasets", "Rows_1": ["Umist", "Coil20", "Jaffe50", "USPS", "Palm", "MSRA50", "Stock", "Pathbased", "Movements", "Spiral", "Wine", "Compound", "Yeast", "Glass", "Ecoli"], "Column_1": ["K-Means", "Ratio Cut", "Normalized Cut", "NMF", "CAN", "PCAN"]}, "indexed_table_256.csv": {"Caption": "Table 2: Results on real life datasets", "Rows_1": ["Karate", "Football"], "Rows_2": ["PR", "Leverage", "LR"], "Column_1": ["NMI", "Jaccard", "RI", "F-measure", "DNC", "ANC"]}, "indexed_table_194.csv": {"Caption": "Table 4: Results on simulated datasets with 300,000 nodes", "Rows_1": ["MCS=50 5", "MCS=50 10", "MCS=50 20", "MCS=500 5", "MCS=500 10", "MCS=500 20", "MCS=5000 20"], "Rows_2": ["PR", "Leverage", "LR"], "Column_1": ["NMI", "Jaccard", "RI", "F-measure", "DNC", "ANC"]}, "indexed_table_228.csv": {"Caption": "Table 3: Comparison of classification accuracy (%) with state-of-art DML methods. \u201cDropout\u201d refers to the best result of dropout from Table 2. Note that LMNN is a batch learning algorithm, and there is no limitation for the triplets it uses and the number of PSD projections. The best result is bolded (statistical significance examined via pairwise t-tests at the 5% significance level). Table 4: Comparison of classification accuracy (%) for SPML and its variants by wrapping different dropout strategies in. The best result is bolded. Table 5: Comparison of classification accuracy (%) for OASIS and its variants by wrapping different dropout strategies in. The best result is bolded.", "Rows_1": ["SPML", "OASIS", "LMNN", "Dropout", "SPML M1", "SPML M2", "SPML D", "OASIS M1", "OASIS M2", "OASIS D"], "Column_1": ["ta", "semeion", "dna", "caltech", "protein", "sensit"]}, "indexed_table_0.csv": {"Caption": "Table 1: The averaged MSE (standard deviation) over 10 random splittings of training and test samples. Training ratio refers to the ratio of training samples to the total samples.", "Rows_1": ["Synthetic", "ADNI ADAS Cog", "ADNI MMSE"], "Rows_2": ["Training 60%", "Training 70%", "Training 80%"], "Column_1": ["training ratio", "Calibration", "Calibration L2 LeastSquares", "LeastSquares L2"]}, "indexed_table_167.csv": {"Caption": "Table 4: Document completion perplexity and PMI for hca variants. Data is presented as \u201cPerplexity/PMI\u201d. \u201cHDP\u201d is short for \u201cHDP-LDA\u201d.", "Rows_1": ["MLT", "MLT", "RML", "RML", "PN", "PN"], "Column_1": ["LDA", "Burst LDA", "HDP", "Burst HDP", "NP LDA", "Burst NP LDA"]}, "indexed_table_81.csv": {"Caption": "Table 5: AUC value comparison", "Rows_1": ["RWRH", "RWRH *", "BIRW", "BIRW *", "PRINCE", "PRINCE*", "Katz", "Katz*", "CrossRank"], "Column_1": ["AUC50", "AUC100", "AUC300", "AUC500", "AUC"]}, "indexed_table_300.csv": {"Caption": "Table 2: Predictive performance obtained using geometric and arithmetic expectations. (The experimental design was identical to that used to obtain the results in table 1.) Using geometric expectations resulted in the same or better perfor- mance than that obtained using arithmetic expectations.", "Rows_1": ["I top 25", "G top 25", "I top 100", "G top 100", "I top 25 c", "G top 25c", "I top100c", "G top100c"], "Column_2": ["MAE", "HAMZ"], "Column_1": ["BPTF ARI", "BPTF GEO"]}, "indexed_table_429.csv": {"Caption": "Table 2: The performance of different graph clustering algorithms on labeled real-world networks.", "Rows_1": ["Attractor", "Ncut", "Modulairty", "Metis", "MCL", "Louvain", "Infomap"], "Column_2": ["NMI", "ARI", "Pur"], "Column_1": ["Zarachy", "Football", "Polbooks", "Amazon"]}, "indexed_table_219.csv": {"Caption": "Table 2: Running time (seconds) on 40 cores with hyper-threading (RT), average relative error (ARE), and correctness ratio (CR) versus k for k-BFS.", "Rows_1": ["com Youtube", "as skitter", "roadNet CA", "wiki Talk", "soc LJ", "cit Patents", "com LJ", "com Orkut", "nlpkkt 240", "Twitter", "com Friendster", "Yahoo", "randLocal", "3D grid"], "Column_2": ["RT", "ARE", "CR"], "Column_1": ["26", "27", "28", "29", "210", "212", "214"]}, "indexed_table_299.csv": {"Caption": "Table 4: Performance of predictions in binary targets", "Rows_1": ["Target", "help family", "go to club", "help others", "depression", "pessimism", "sleep", "irritability", "appetite", "concentration", "orientation to date", "numeracy"], "Column_2": ["Linkage", "baseline"], "Column_1": ["AUC", "Prediction Accuracy"]}, "indexed_table_62.csv": {"Caption": "Table 2: Results of text classification on long documents.", "Rows_1": ["Name", "Train", "Test", "|V|", "Doc length", "#classes"], "Column_2": ["Wiki", "IMDB", "Corporate", "Economics", "Government", "Market", "Dblp", "Mr", "Twitter"], "Column_1": ["Long Documents", "Short Documents"]}, "indexed_table_329.csv": {"Caption": "Table 4: The classification result in terms of MAP (mean average precision) for the NUS-WIDE dataset.", "Rows_1": ["Sample", "Image only", "Text only", "Image Text", "CCA", "DT", "LHNE", "HNE"], "Column_2": ["CCA", "DT", "LHNE", "HNE"], "Column_1": ["rank 1", "rank 2", "rank 3", "rank 4"]}, "indexed_table_137.csv": {"Caption": "Table 3: APR of our models in different time slots.", "Rows_1": ["Cstatic", "C", "R", "RCHC", "RCHBAC"], "Column_2": ["12-4 am", "4-8 am", "8-12 pm", "12-4 pm", "4-8 pm", "8-12 am"], "Column_1": ["Workdays", "Holidays"]}, "indexed_table_422.csv": {"Caption": "Table 3: Prediction accuracies for competing algorithms with different forecast steps over different countries using the weather data source.", "Rows_1": ["ARX", "MFN", "DARX", "DPARX"], "Column_1": ["AR", "BO", "CL", "CO", "CR", "EC", "GT", "HN", "MX", "NI", "PA", "PE", "PY", "SV", "US"]}, "indexed_table_416.csv": {"Caption": "Table 1: Performance comparison of different organizational chart inference methods.", "Rows_1": ["Create", "Create", "Create", "Create sl", "Create sm", "Create sm", "Create sm", "Create s", "S CN", "S JC", "S AA"], "Column_1": ["AUC", "Precision@100"]}, "indexed_table_61.csv": {"Caption": "Table 2: Performance comparison in terms of accuracy, sensitivity, specificity, and AUC achieved by CNN models and Sparse Coding features for all stage ranges. \u201cTL+MTL\u201d and \u201cTL\u201d denote the features extracted from layer 21 of the deep model for multi-task learning and transfer learning. \u201cSC\u201d denotes the performance of the sparse coding features.", "Rows_1": ["Accuracy", "Sensitivity", "Specificity", "AUC"], "Rows_2": ["TL MTL", "TL", "SC", "TL MTL"], "Column_1": ["Stage 4-6", "Stage 7-8", "Stage 9-10", "Stage 11-12", "Stage 13-17"]}, "indexed_table_216.csv": {"Caption": "Table 2: Performance comparison of different methods for network integration task. The results are presented jointly and separately for each pair of networks .", "Rows_1": ["Network Pair", "Twitter LiveJournal", "Twitter Flickr", "Twitter Last fm", "Twitter MySpace", "LiveJournal Flickr", "LiveJournal Last fm", "LiveJournal MySpace", "Flickr Last fm", "Flickr MySpace", "Last fm MySpace", "Overall", "ArnetMiner LinkedIn", "ArnetMiner VideoLectures", "LinkedIn VideoLectures"], "Column_2": ["Prec", "Rec", "F1"], "Column_1": ["Name match", "SVM", "MNA", "SiGMa", "COSNET", "COSNET"]}, "indexed_table_66.csv": {"Caption": "Table 7: Truth Inference", "Rows_1": ["TEM", "Vot", "VotE", "Find", "FindE", "Ave", "AveE", "Inv", "InvE", "PInv", "PInvE", "3Est", "3EstE", "LCA", "LCAE", "LTM EM"], "Column_2": ["PREC", "REC", "F1"], "Column_1": ["SF 2013", "SF 2014", "Flight"]}, "indexed_table_345.csv": {"Caption": "Table 10: Readability of results.", "Rows_1": ["Kincaid", "ARI", "Coleman Liau", "Flesch Index", "Fog Index", "SMOG Grading"], "Column_1": ["Google", "Social Pulse"]}, "indexed_table_242.csv": {"Caption": "Table 10: Readability of results.", "Rows_1": ["Regression Tree", "Nearest Neighbors", "FFA Neural Nets", "Polynomial Regression"], "Column_1": ["MAE Valid", "RAE Valid", "MAE Test"]}, "indexed_table_277.csv": {"Caption": "Table 4: Expected results if companies A and B can only contact 20% of leads.", "Rows_1": ["A", "B"], "Rows_2": ["Random", "DQM", "FFM"], "Column_1": ["Conversion rate", "Close rate", "Total revenue"]}, "indexed_table_350.csv": {"Caption": "Table 1: Highest R@K achieved by each one of the considered algorithms (over the test set) after one training pass. R@K is reported in terms of relative improvement (in percentage) over the values achieved by ranking the recommendations uniformly at random. ElasticRank (listwise) is more accurate than RankSVM (pairwise), which in turn outperforms LR (pointwise).", "Rows_1": ["LR", "RankSVM", "PSGD", "FOBOS", "RDA"], "Column_1": ["R@1", "R@2", "R@3", "R@4", "R@5", "NDCG"]}, "indexed_table_187.csv": {"Caption": "Table 3: Solution time and action costs of various methods. All results are averaged over 100(C-1) runs. Optimal costs are marked in bold. See text for details of experimental setup.", "Rows_1": ["a1a", "liver disorders", "australian", "breast cancer", "dna", "glass", "heart", "ionosphere", "mushrooms"], "Column_2": ["Cost"], "Column_1": ["Iterative testing", "Greedy", "Cost aware Greedy", "ILP method"]}, "indexed_table_24.csv": {"Caption": "Table2:Ob ject detection/classification accuracy", "Rows_1": ["Objects", "shoe", "dress", "glasses", "bag", "watch", "pants", "shorts", "bikini", "earrings", "Average"], "Column_2": ["TP", "FP"], "Column_1": ["Text", "Img", "Both"]}, "indexed_table_14.csv": {"Caption": "Table 4: The performance of AUPR on different methods.", "Rows_1": ["CN", "AA", "JC", "PA", "PF", "IT", "LRC IT", "LRC", "DT IT", "DT", "CoupledLP IT", "CoupledLP"], "Column_1": ["D to G", "G to D", "Aa to Ab", "Ab to Aa", "Ea to Eb", "Eb to Ea", "Ea to Ec", "Ec to Ea", "Eb to Ec", "Ec to Eb"]}, "indexed_table_183.csv": {"Caption": "Table 2: Regression Results", "Rows_1": ["Ridge Regression", "SVM", "Neural Networks"], "Column_2": ["Train", "Validation"], "Column_1": ["IN Set", "PH Set"]}, "indexed_table_157.csv": {"Caption": "Table 1: Performance of Classifiers on Holdout Data.", "Rows_1": ["AUC", "Accuracy", "Precision", "Recall"], "Column_2": ["Features Onl", "Classifier"], "Column_1": ["Length", "NLP", "Longform"]}, "indexed_table_423.csv": {"Caption": "Table 1: Online over-time test result of 4 real campaigns (# layers: 3).", "Rows_1": ["1", "2", "3", "4"], "Column_2": ["AvgPR", "Spending", "Omega", "AvgErr", "eCPC", "eCPC reductio"], "Column_1": ["Smart Pacing"]}, "indexed_table_94.csv": {"Caption": "Table 1: Low-accuracy annotator performance. The estima- tors are trained on dataset that is not used anywhere else in the rest of the experiments.", "Rows_1": ["LogRegL2", "LogRegL1", "CART", "MARS", "GBM", "RandFor", "SVM"], "Column_1": ["precision", "recall", "F1", "AUC", "runtime"]}, "indexed_table_328.csv": {"Caption": "Table 2: Offline Performance", "Rows_1": ["TFIDF", "Chi Square Test", "Independent Learning Model", "Latent Factor Model"], "Column_1": ["MAP", "MRR", "AUC"]}, "indexed_table_362.csv": {"Caption": "Table 2: Effects of parameter settings on PC, RR and F-Measure for Cora, UKCD, and NCVR-450, showing different configurations of smin, smax and the three different block similarity measures (_) single link, average link, and complete link. The best value(s) in each row is shown in bold.", "Rows_1": ["smin  smax", "Size based", "Similarity Based"], "Rows_2": ["Block similarity measure", "PC", "FM"], "Column_2": ["Block similarity measure", "Single", "Average", "Complete"], "Column_1": ["Cora 20-50", "Cora 20-100", "Cora 50-100"]}, "indexed_table_237.csv": {"Caption": "Table 1: Overall Comparison", "Rows_1": ["recall"], "Column_1": ["Flickr 1M", "Flickr 2M", "Flickr 3M", "Flickr 4M", "Flickr 5M", "Flickr 6M", "Flickr 7M", "MilionSong"], "Column_2": ["OPT", "SH", "MLSH", "SLSH", "LLSH", "DSH", "ISO"]}, "indexed_table_22.csv": {"Caption": "Table 2: Operative Mortality: Comparing Target + Source (Unweighted) to Random, Target-only, Source- only, and STS baselines. Estimated precision, recall, and AUC are shown for Isolated AVR, Isolated MVR, and Isolated MV Repair procedures at each hospital. The best precision, recall, and AUC are in bold. Random precision (incidence in the population), recall (0.20), and AUC (0.50) are shown in the top row for each experiment. The Target-only performance for Isolated MV Repair at Hospital 1 is missing because there were not enough positive examples to develop a model (only two adverse events). Target + Source (Unweighted) performance is better than Target-only performance on all tasks, and it is comparable to STS performance for all six tasks (p-values from McNemar\u2019s test > 0.05).", "Rows_1": ["Random", "Target only", "Source only", "Target Source", "STS"], "Column_2": ["Precision", "Recall", "AUC"], "Column_1": ["Hospital 1", "Hospital 2"]}, "indexed_table_2.csv": {"Caption": "Table 4: Operative Mortality: Comparing performance of weighted methods versus Target + Source (Un- weighted). Estimated precision, recall, and AUC are shown for Isolated AVR, Isolated MVR, and Isolated MV Repair procedures at each hospital. The best precision, recall, and AUC are in bold. McNemar\u2019s test p-values for differences between weighted methods and Target + Source (Unweighted) are shown for each task.", "Rows_1": ["Target Source", "KLIEP weighted", "Weighted Instances", "Weighted Clusters K=1", "Weighted Clusters K=2", "Weighted Clusters K=3"], "Column_2": ["Precision", "Recall", "AUC", "McNemar"], "Column_1": ["Hospital 1", "Hospital 2"]}, "indexed_table_395.csv": {"Caption": "Table 5: Prolonged Length of Stay (LOS > 14 days): Comparing performance of weighted methods versus Target + Source (Unweighted). Estimated precision, recall, and AUC are shown for Isolated AVR, Isolated MVR, and Isolated MV Repair procedures at each hospital. The best precision, recall, and AUC are in bold.", "Rows_1": ["Target Source", "KLIEP weighted", "Weighted Instances", "Weighted Clusters K=1", "Weighted Clusters K=2", "Weighted Clusters K=3"], "Column_2": ["Precision", "Recall", "AUC", "McNemar"], "Column_1": ["Hospital 1", "Hospital 2"]}, "indexed_table_449.csv": {"Caption": "Table 2: Average prediction performance of different methods on the Coauthor and Weibo datasets. The numbers enclosed in brackets are standard deviations.", "Rows_1": ["Coauthor", "Weibo"], "Rows_2": ["SVM", "SMO", "LR", "NB", "RBF", "C4.5", "CRM"], "Column_1": ["Precision", "Recall", "F1 measure", "AUC"]}, "indexed_table_84.csv": {"Caption": "Table 4: Average accuracy of each testing method when the sample size bound N is elapsed.", "Rows_1": ["1", "2", "3"], "Column_2": ["BF", "LA", "LS", "LALS", "LALS+"], "Column_1": ["Baseline", "Proposal"]}, "indexed_table_190.csv": {"Caption": "Table 10: Clustering Quality (in Terms of Rand Index) of TADPole vs. Some State-of-the-Art Clustering Algorithms", "Rows_1": ["CBF", "FacesUCR", "MedicalImages", "Symbols", "uWaveGesture_Z"], "Column_2": ["DTW version"], "Column_1": ["TADPoleDTW", "k-means Hierarchical", "DBSCAN", "Spectral"]}, "indexed_table_424.csv": {"Caption": "Table 1: AUROC for classification.", "Rows_1": ["Subsequence", "Episode"], "Rows_2": ["Categories", "Labels"], "Column_1": ["No Prior", "Co Occurrence", "ICD Tree"]}, "indexed_table_143.csv": {"Caption": "Table 1: The evaluation results based on three different measures on real-world data sets. The reported results are averaged values over 20 runs. The best performance values are shown in bold.", "Rows_1": ["VAST InfoVis", "Four Area", "Four Area", "Four Area", "Four Area", "Four Area", "Four Area"], "Column_2": ["StandardNMF", "Batchprocessing", "Pseudodeflation"], "Column_1": ["Reconstruction error", "Commonality score", "Distinctiveness score"]}, "indexed_table_92.csv": {"Caption": "None", "Rows_1": ["20", "30", "40", "10"], "Column_2": ["Q1", "Q2", "Q3", "Q4", "M"], "Column_1": ["TOPTRAC", "LGTA", "NAIVE"]}, "indexed_table_16.csv": {"Caption": "Table 3: Accuracy and Area-Under-the-Curve (AUC) scores for predicting labels at the group (document) level for the baselines and our proposed method (GICF). Training is always done at the group level. Testing on sentences corresponds to scoring each sentence separately and aggregating the results. BOW or embeddings corresponds to the features used.", "Rows_1": ["Logistic BOW on Documents", "Logistic BOW on Sentences", "Logistic Embeddings on Documents", "GICF Embeddings on Sentences"], "Column_2": ["Amazon", "IMDb", "Yelp"], "Column_1": ["Accuracy", "AUC"]}, "indexed_table_290.csv": {"Caption": "Table 2: Average fraction of null hypothesis rejection for different combinations of validation procedures and statistical tests, aggregated over all datasets. The first column group concerns Type I errors, and the other two column groups concern Type II errors.", "Rows_1": ["McNemar non prequential", "McNemar prequential", "Sign test non prequentiall", "Sign test prequential", "Wilcoxon non prequential", "Wilcoxon prequential", "Avg time non prequential", "Avg time prequential"], "Column_2": ["bootstrap", "cv", "split"], "Column_1": ["No change", "Change pnoise = 0.05", "Change pnoise = 0.10"]}, "indexed_table_312.csv": {"Caption": "Table 3: Fraction of null hypothesis rejection on each dataset when using McNemar\u2019s test. Values for the best performing validation method (closest to zero for Type I errors, closest to one for Type II errors) are shown in bold.", "Rows_1": ["CovType", "Electricity", "Poker", "LED", "SEA", "SEA", "HYP", "HYP", "RBF", "RBF", "RBF", "RBF", "RBF", "Average"], "Column_2": ["bootstrap", "cv", "split"], "Column_1": ["No change", "Change pnoise = 0.05", "Change pnoise = 0.10"]}, "indexed_table_400.csv": {"Caption": "Table 2: Performance Comparison", "Rows_1": ["DynaTD", "DynaTD smoothing", "DynaTD decay", "DynaTD All", "Mean", "Median", "GTM", "CRH", "CATD", "TruthFinder", "AccuSim", "Investment", "Estimates"], "Column_2": ["MAE", "RMSE", "Time"], "Column_1": ["Weather Dataset", "Stock Dataset", "Flight Dataset"]}, "indexed_table_18.csv": {"Caption": "Table 2: Performance of the 3 variations of our model, 3 different methods used in the literature and 2 static classification methods", "Rows_1": ["Apache III", "SAPS II", "Random Forests", "Ghasemi 20141", "Naive Bayes", "Proposed method Numerical Features", "Proposed method Text and Numerical", "Proposed method"], "Column_2": ["Sensitivity", "Specificity", "AUC"], "Column_1": ["24 hours", "48 hours", "72 hours"]}, "indexed_table_88.csv": {"Caption": "Table 1: Clustering results of seven datasets in terms of accuracy and NMI. N is the number of data points. c is the true number of clusters. D is the original dimensionality and d is the reduced dimensionality.", "Rows_1": ["Accuracy", "NMI"], "Rows_2": ["K-means", "PCA K-means", "LLE K-means", "Laplacian K-means", "DT K-means", "DDRT K-means"], "Column_1": ["Iris", "Letter", "Vehicle", "Glass", "Segment", "USPS", "Pendigits"]}, "indexed_table_410.csv": {"Caption": "Table 2: Accuracy of different methods on synthetic datasets", "Rows_1": ["view", "dom"], "Rows_2": ["SNMF", "Spectral", "CTSC", "PairCRSC", "CentCRSC", "TF", "CGC", "NoNClus"], "Column_1": ["Main cluster 1", "Main cluster 2", "Main cluster 3"], "Column_2": ["Net 1", "Net 2", "Net 3", "Net 4", "Net 5", "Net 6", "Net 7", "Net 8", "Net 9", "Net 10", "Overall"]}, "indexed_table_119.csv": {"Caption": "Table 4: Comparison of averaged AUCs among various methods. In all the settings, our proposed method outperformed all the other methods in terms of AUCs. We conducted the Wilcoxon signed rank test for each pair of our proposed method and compared method, and confirmed that our proposed method statistically significantly (p < 0.05) outperformed all the other methods in each setting, except for the bold-faced results.", "Rows_1": ["Proposed", "Proposed feat", "Proposed dz", "non MTL 1", "non MTL 2", "MTL 1", "MTL 2"], "Column_2": [], "Column_1": ["Pre ICU discharge prediction", "Retrospective prediction"]}, "indexed_table_356.csv": {"Caption": "Table7:  Efficiency comparison of the algorithms. DNF means an algorithm did not finish in 10,000 seconds in average.", "Rows_1": ["amazon 0601", "as Skitter", "cit Patents", "com DBLP", "email Enron", "soc Epinions 1"], "Column_2": [], "Column_1": ["This work", "Chien et al", "Bahmani et al", "Warm start", "From scratch"]}, "indexed_table_201.csv": {"Caption": "Table 4: Computation time [sec] of model selection based on LOOCV (without tricks).", "Rows_1": ["D1 linear", "D1 nonlinear", "D2 linear", "D2 nonlinear", "D3 linear", "D3 nonlinear", "D4 linear", "D4 nonlinear"], "Column_2": ["existing incrementa", "existing [19", "existing [10", "proposed op", "proposedop1", "proposedop2"], "Column_1": ["SVM", "Logistic regression"]}, "indexed_table_134.csv": {"Caption": "Table 3: Retrieval performance evaluated by MAP and corresponding Hamming Radius to Search (HRS). We retrieve 500 nearest entities for evaluation. For MuCH-S, the bit number in second row represents the bit number of single hash table, and the number of hash tables is 4. For MuCH-F, the bit number in second row represents the total bit number of all hash tables, and the number of hash tables is 2.", "Rows_1": ["MuCH S", "MuCH F", "KSH", "SSH", "ITQ"], "Column_2": ["MAP", "HRS"], "Column_1": ["NUS WIDE 16 bits", "NUS WIDE 32bits", "DBLP 16 bits", "DBLP 32 bits"]}, "indexed_table_133.csv": {"Caption": "Table 5: Performance comparisons on three datasets in terms of Precision, Recall and F1 score.", "Rows_1": ["Pattern", "FIGER", "SemTagger", "APOLLO", "NNPLB", "ClusType NoClus", "ClusType NoWm", "ClusType TwoStep", "ClusType"], "Column_2": ["Precision", "Recall", "F1"], "Column_1": ["NYT", "Yelp", "Tweet"]}, "indexed_table_246.csv": {"Caption": "Table 2: Evaluation of multi-label classification with different penalty functions, using 10% of labeled data for training.", "Rows_1": ["LWD", "LWRD", "LWRLD"], "Rows_2": ["Hamming Sc", "Micro F1 Sc", "Macro F1 Sc"], "Column_1": ["Youtube", "PubMed", "CoRA", "IMDb"]}, "indexed_table_145.csv": {"Caption": "Table 2: Event forecasting performance comparison based Accuracy (Acc) and F-1 score w.r.t to state-of-the-art methods. The proposed nMIL , nMIL_, nMIL\u03a9method outperform state-of-the-art methods across the three countries.", "Rows_1": ["SVM", "MI SVM", "rMILnor", "rMILavg", "GICF", "nMIL", "nMIL_"], "Column_2": ["Acc", "F1"], "Column_1": ["Argentina", "Brazil", "Mexico"]}, "indexed_table_297.csv": {"Caption": "Table 3: MAP of vertex recommendation on SN-TWeibo and SN-Twitter. For each vertex, the recommended vertex list is ranked according to the predicted proximity between vertexes. For embedding algorithms, we calculate the predicted proximity by performing inner product between embedding vectors.", "Rows_1": ["HOPE", "PPE", "LINE1", "LINE2", "DeepWalk", "Common Neighbors", "Adamic Adar"], "Column_2": ["MAP@10", "MAP@50", "MAP@100"], "Column_1": ["SN TWebio", "SN Twitter"]}, "indexed_table_31.csv": {"Caption": "Table 8: M-MLkNN versus MLkNN on hamming loss, ranking loss, coverage, one error and average precision. Each result is an average over 10 trials of 10-fold cross-validation and its standard error. For the first four performance measures, the lower the better; for the last one, the higher the better.", "Rows_1": ["Birds", "CAL500", "Emotions", "Enron", "Scene"], "Rows_2": ["MLkNN", "M MLkNN"], "Column_1": ["HammingLoss RankingLoss", "Coverage", "OneError", "AveragePrecision"]}, "indexed_table_195.csv": {"Caption": "Table 5: precision@k on ARXIV GR-QC for link prediction", "Rows_1": ["SDNE", "LINE", "DeepWalk", "GraRep", "Common Neighbor", "LE"], "Column_1": ["P@2", "P@10", "P@100", "P@200", "P@300", "P@500", "P@800"]}, "indexed_table_287.csv": {"Caption": "Table 3: Precisions of setting one. The last two rows are (a) the average scores of all targeted aspects of all topics and (b) the improvement achieved by TTM over other models respectively", "Rows_1": ["children", "E Cig vape", "health", "box", "Cigar event", "horse", "screen", "Camera lens", "weight", "sound", "Cell Phone case", "battery", "monitor", "Computer software", "warranty", "average score"], "Column_2": ["P@5", "P@10", "P@20"], "Column_1": ["LDA", "SS LDA", "LDA PD", "TTM"]}, "indexed_table_117.csv": {"Caption": "Table 4: Precisions of setting two. The last two rows are (a) the average scores of all targeted aspects of all topics and (b) the improvement achieved by TTM over other models respectively.", "Rows_1": ["children", "E Cig vape", "health", "box", "Cigar event", "horse", "screen", "Camera lens", "weight", "sound", "Cell Phone case", "battery", "monitor", "Computer software", "warranty", "average score"], "Column_2": ["P@5", "P@10", "P@20"], "Column_1": ["LDA", "LDA *", "DS LDA *", "SS LDA *"]}, "indexed_table_106.csv": {"Caption": "Table 2: Experimental results comparison of clustering on benchmark datasets.", "Rows_1": ["ACCURACY", "NMI"], "Rows_2": ["AR", "FERET", "Yale", "ORL", "Carcinomas", "SRBCTML", "LEUML"], "Column_1": ["Ratio Cut", "Normalized Cut", "DSN", "SDS"]}, "indexed_table_235.csv": {"Caption": "Table 2: nDCG@k of multi-label learning algorithms on the Bibtex, Delicious-S, Mediamill datasets.", "Rows_1": ["KNN", "OneVsAll", "LPSR", "ML CSSP", "CS", "CPLST", "WSABIE", "LEML", "SLEEC", "REML"], "Column_2": ["nDCG@1", "nDCG@3", "nDCG@5"], "Column_1": ["Delicious S", "Mediamill"]}, "indexed_table_203.csv": {"Caption": "Table 1: Description of the databases: number of series of events; minimum, average, and maximum number of events; classification test results; gaussian fit parameters.", "Rows_1": ["AskMe", "Digg", "Enron", "MetaFilter", "MetaTalk", "Reddit", "Twitter", "Yelp"], "Column_2": ["Min", "Avg", "Max", "Mix", "PP", "SFP", "Log"], "Column_1": ["Min Avg Max", "Hypothesis Test", "Bivariate Gaussian"]}, "indexed_table_248.csv": {"Caption": "Table 3: Clustering NMI of the diagnosis, medication and procedure code representations of various models. All mod- els learned 200 dimensional code vectors. All models except SVD were trained for 10 epochs.", "Rows_1": ["SVD", "Skip gram", "GloVe", "Med2Vec"], "Column_1": ["Diagnosis", "Medication", "Procedure"]}, "indexed_table_48.csv": {"Caption": "Table 2: Reordering results of various algorithms on graphs: the costs of MLogA, BiMLogA, and the number of bits per edge required by BV. The best results in every column are highlighted. We present the results that completed the computation within a few hours.", "Rows_1": ["Enron", "AS Oregon", "FB NewOrlean", "web Google", "LiveJournal"], "Rows_2": ["Natural", "BFS", "Minhash", "TSP", "LLP", "Spectral", "Multiscale", "SlashBurn", "BP"], "Column_1": ["LogGap", "Log", "BV"]}, "indexed_table_325.csv": {"Caption": "Table 2: Accuracy for each classifier / dataset (%).", "Rows_1": ["A", "B", "C", "D", "E", "F"], "Column_2": ["BL1", "BL2", "MR", "AB", "TL", "MRP", "CD", "AT"], "Column_1": ["Nearest Neighbor", "Naive Bayes", "Decision Tree"]}, "indexed_table_415.csv": {"Caption": "Table 2: Clustering Performance of Different Algorithms Measured by Accuracy", "Rows_1": ["letter", "MNIST", "COIL100", "Amazon", "Caltech", "Dslr", "Webcam", "ORL", "USPS", "Caltech101", "ImageNet", "Sun09", "VOC2007"], "Column_2": ["K-means", "MAEC1", "GCC", "HCC", "KCC", "SEC", "IEC"], "Column_1": ["Baseline", "Deep Clustering Method", "Ensemble Clustering Method"]}, "indexed_table_177.csv": {"Caption": "Table 9: Study of performance improvement on fine-grained typing systems FIGER [14] and HYENA [35] on the three datasets.", "Rows_1": ["Noise Reduction Method", "PL SVM", "CLPL", "Raw", "Min", "All", "WSABIE Min", "PTE Min", "PLE NoCo", "PLE", "Min", "All", "WSABIE Min", "PTE Min"], "Column_2": ["Acc", "Ma-F1", "Mi-F1"], "Column_1": ["Wiki", "OntoNotes", "BBN"]}, "indexed_table_173.csv": {"Caption": "Table 5: Performance comparison on Stock data", "Rows_1": ["Day 1", "Day 2", "Day 3", "Day 4", "Day 5", "Day 6", "Day 7", "Day 8", "Day 9", "Day 10", "Day 11", "Day 12", "Day 13", "Day 14", "Day 15", "Day 16", "Day 17", "Day 18", "Day 19"], "Column_2": ["RGMM", "CRH", "CATD", "GTM", "Mean", "Median"], "Column_1": ["MAE", "RMSE"]}, "indexed_table_322.csv": {"Caption": "Table 2: Comparison on simulated data: all scenarios", "Rows_1": ["ETCIBoot", "CATD", "CRH", "Median", "Mean", "GTM"], "Column_2": ["MAE RMSE"], "Column_1": ["Scenario 1", "Scenario 2", "Scenario 3", "Scenario 4"]}, "indexed_table_4.csv": {"Caption": "Table 2: Relative CTR on Yahoo! News Data.", "Rows_1": ["greedy", "greedy", "greedy", "greedy", "Bootstrap", "Bootstrap", "Bootstrap", "Bootstrap", "LinUCB", "LinUCB", "LinUCB", "LinUCB", "LinUCB", "TS", "TS", "TS", "TS", "TS", "TSNR", "TSNR", "TSNR", "TSNR", "TSNR", "TSNR"], "Column_2": ["mean", "std", "min", "max"], "Column_1": ["Logistic Regression", "Linear Regression"]}, "indexed_table_405.csv": {"Caption": " None", "Rows_1": ["Mean Probability Score", "Mean Quality Score", "Recall", "Precision"], "Column_2": ["1 day", "3 day", "7 day"], "Column_1": ["Month 12", "Month 24", "Month 36"]}, "indexed_table_109.csv": {"Caption": "Table 4: Performances of portfolio selections.", "Rows_1": ["Non default", "Fully funded", "Winning bid", "Return", "Return"], "Column_1": ["DPA 1", "DPA 2", "DPA 3", "DPA 4", "EVA 1", "EVA 2", "EVA 3", "EVAR"]}, "indexed_table_236.csv": {"Caption": "Table 2: Classification results of five-fold cross validation.", "Rows_1": ["PTM", "SPTM", "SATM", "LDA", "DSTM", "MU"], "Column_2": ["precision", "recall", "f-measure"], "Column_1": ["News", "DBLP", "Questions", "Tweets"]}, "indexed_table_176.csv": {"Caption": "Table 4: Classification results of PTM, SPTM and EPTM.", "Rows_1": ["PTM", "SPTM", "EPTM", "LDA", "DSTM", "MU"], "Column_2": ["precision", "recall", "f-measure"], "Column_1": ["News", "DBLP", "Questions", "Tweets"]}, "indexed_table_337.csv": {"Caption": "Table 3: Results of LASSO on the test set - Question-wise results", "Rows_1": ["Question Name", "is Same Reflection", "waiting Time SJF", "count Cache Miss", "is Tree", "pattern Print", "gray Check", "transpose MultMatrix", "eliminate Vowel String", "cell Compete", "insert Sorted List", "is SubTree", "min Tree Path", "is Path", "lru Count Miss", "generalized GCD", "distinct Element Count", "rotate Picture Method", "reverse Linked List", "balanced Parentheses"], "Column_2": ["Correl", "Bias", "MAE"], "Column_1": ["QuesIndep N1", "Baseline TC"]}, "indexed_table_261.csv": {"Caption": "Table 1: The accuracy, precision, recall, and false positive rate (FPR) for each purchase cohort and overall for our stage 1 model. All results are per- centages.", "Rows_1": ["Unactivated", "New users", "One time buyers", "Sporadic buyers", "Power users", "Overall"], "Column_1": ["Accuracy", "Precision", "Recall", "FPR"]}, "indexed_table_207.csv": {"Caption": "Table 5: The performance of different methods.", "Rows_1": ["D#1", "D#2", "D#3"], "Rows_2": ["LR", "Ridge", "Lasso", "LsLR", "DT", "RF", "SVR", "MLR DOM"], "Column_1": ["rMSE", "nMSE", "MAE"]}, "indexed_table_450.csv": {"Caption": "Table 3: Precision, Recall, F-score, Deviation of different classifiers - median on 100 runs , each of which using randomly-drawn training and test data points", "Rows_1": ["Replaced Precision", "Replaced Recall", "Replaced F1", "Replaced Sd", "Healthy Precision", "Healthy Recall", "Healthy F1", "Healthy Sd"], "Column_2": ["SgtA", "HitA"], "Column_1": ["RGF", "GBDT", "RF", "SVM", "LR", "DT"]}, "indexed_table_146.csv": {"Caption": "Table 6: Performance comparison on the aca- demic institution datasets (Cov means coverage; Pre means precision; SR means success rate).", "Rows_1": ["4icu uk", "guardian", "s40 max", "s7-39", "s-16"], "Rows_2": ["sCooL", "CompanyDepot H"], "Column_1": ["Cov", "Pre", "SR", "F1"]}, "indexed_table_131.csv": {"Caption": "Table 6: Quality and Coverage Evaluation for AutoGSR vs Manual GSR*.", "Rows_1": ["Argentina", "Brazil", "Chile", "Colombia", "Ecuador", "El Salvador", "Mexico", "Paraguay", "Uruguay", "Venezuela"], "Column_2": ["Oct", "Nov", "Dec"], "Column_1": ["Quality Score", "Precision", "Recall"]}, "indexed_table_311.csv": {"Caption": "Table 6: Estimated p-value for each feature. The p-value is de- fined as the possibility that a smaller error measure is observed under the null hypothesis.", "Rows_1": ["D", "G", "P", "T"], "Column_1": ["LR", "NB"], "Column_2": ["MAE", "MRE"]}, "indexed_table_122.csv": {"Caption": "Table 9: Online A/B testing of bid optimisation (Yahoo!).", "Rows_1": ["Camp", "C1", "C2", "C3", "C4", "C5", "C6", "C7", "C8", "C9", "all"], "Column_2": ["bias", "kmmp"], "Column_1": ["Impressions", "Clicks", "CTR", "eCPC"]}, "indexed_table_336.csv": {"Caption": "Table 3: The columns on the left show the sizes of the sketches and the average ADS retrieval time, and the columns on the right show the time and memory consumption for construction (k = 16).", "Rows_1": ["wiki Vote", "email Enron", "web NotreDame", "com dblp", "web Google", "com youtube", "in 2004", "flickr links", "soc LiveJournal1", "hollywood 2009", "indochina 2004", "com orkut"], "Column_2": ["ADS", "ADS", "SRS", "SRS", "SRS"], "Column_1": ["Sketch size", "Construction space"]}, "indexed_table_204.csv": {"Caption": "Table 2: Average inter-sequence distance (ISD), average no. containing sequences (CS) and no. unique items for the top 50 non-singleton sequences returned by the algorithms from the datasets. Larger inter-sequence distances and smaller no. containing sequences indicate less redundancy. * returned less than 50 non-singleton sequences.", "Rows_1": ["ISM", "SQS", "GoKrimp", "BIDE"], "Column_2": ["ISD", "CS", "Items"], "Column_1": ["Alice", "Gazelle", "JMLR", "Sign", "aslbu"]}, "indexed_table_147.csv": {"Caption": "Table 3: Structural hole spanner detection results (average SHII). Column 2 indicates the used Information Diffusion model", "Rows_1": ["Karate Club", "DBLP", "YouTube"], "Rows_2": ["LT", "IC", "SH spanners"], "Column_1": ["HAM", "Constraint", "PageRank BC Step", "MaxD", "HIS", "AP BICC"]}, "indexed_table_357.csv": {"Caption": "Table 1: Performance comparison between the un- supervised baselines and the FTA.", "Rows_1": ["Accuracy", "Specificity", "Sensitivity", "J Statistics", "Time"], "Column_1": ["DTW", "DDTW", "CTW", "FTA"]}, "indexed_table_47.csv": {"Caption": "None", "Rows_1": ["Sites", "KKI", "NeuroImage", "OHSU", "Peking", "Average"], "Column_2": ["AGDM", "FTA"], "Column_1": ["Prediction Accuracy", "Specificity", "Sensitivity", "J Statistic"]}, "indexed_table_43.csv": {"Caption": "Table 1: Comparison of precision, recall, and F1 for three models on Douban, Gowalla, Epinions", "Rows_1": ["GT", "VAREM", "SIG"], "Column_2": ["Precision", "Recall", "F1 Score"], "Column_1": ["Douban", "Gowalla", "Epinions"]}, "indexed_table_266.csv": {"Caption": "Table2:Dataset statistics", "Rows_1": ["EUR Lex", "AmazonCat 13K", "Wiki10 31K", "WikiLSHTC 325K", "Amazon 670K", "Ads 9M"], "Column_2": ["N", "D", "L", "M", "per poin", "per labe"], "Column_1": ["Train", "Features", "Labels", "Test", "Avg labels", "Avg points"]}, "indexed_table_373.csv": {"Caption": "Table 3: Response ranking", "Rows_1": ["Frequency", "Multiclass BOW", "Smart Reply", "Unique Clusters", "Unique Suggestions"], "Column_1": ["Precision@10", "Precision@20", "MRR"]}, "indexed_table_164.csv": {"Caption": "Table 3: Comparison of constraints with replay and A/B test results", "Rows_1": ["All", "1", "2", "3", "Rest"], "Column_2": ["send", "complaint", "session", "complaint", "session", "complaint", "session"], "Column_1": ["Contribution", "Constraint", "MOO Coeff", "Offline Replay", "A/B Test"]}, "indexed_table_232.csv": {"Caption": "Table 1: The performance of Reasoning Network on CNN and Daily Mail dataset.", "Rows_1": ["Deep LSTM Reader", "Attentive Reader", "MemNets", "AS Reader", "DER Network", "Iterative Attention Reader", "EpiReader", "GA Reader", "AoA Reader", "ReasoNet"], "Column_2": ["valid", "test"], "Column_1": ["CNN", "Daily Mail"]}, "indexed_table_247.csv": {"Caption": "Table 2: Results on the SQuAD test leaderboard.", "Rows_1": ["Logistic Regression Baseline", "Dynamic Chunk Reader", "Fine Grained Gating", "Match LSTM", "RaSoR", "Multi Perspective Matching", "Dynamic Coattention Networks", "BiDAF", "ReasoNet", "Iterative Co attention Network", "FastQA", "jNet", "Document Reader", "R Net"], "Column_2": ["EM", "F1"], "Column_1": ["Single Model", "Ensemble Model"]}, "indexed_table_189.csv": {"Caption": "Table 3: Classification performance of compared methods", "Rows_1": ["BOW", "Skip gram", "LINE", "PTE", "CNN", "RNN", "FastText", "BOWRF", "Skip gramRF", "LINERF", "PTERF", "MemNet", "ExpaNet S", "ExpaNet H"], "Column_2": ["Micro-F1", "Macro-F1"], "Column_1": ["Wikipedia", "Dblp", "Twitter"]}, "indexed_table_249.csv": {"Caption": "Table 4: Performance comparison of expansion using long documents from general domain vs. speci c domain", "Rows_1": ["BOW RF", "Skip gram RF", "LINE RF", "PTE RF", "MemNet"], "Column_2": ["Micro-F1", "Macro-F1"], "Column_1": ["Dblp memory", "Wikipedia memory"]}, "indexed_table_278.csv": {"Caption": "Table 2: Outcome: In-Hospital Mortality. Di erence in AUC between SAPS II + Item IDs and SAPS II + CUIs (Span- ning) shown. Statistical Signi cance evaluated using the Wilcoxon Signed-Rank Test.", "Rows_1": ["Gap", "0", "12", "24", "36", "48"], "Column_2": ["Mean Difference in AUC", "p value"], "Column_1": ["CareVue", "MetaVision"]}, "indexed_table_75.csv": {"Caption": "Table 2: Classi cation accuracy on a validation dataset from a di erent network for both the standard and enhanced feature sets. Malware data from September is included to demonstrate majority-class classi ers.", "Rows_1": ["LinReg", "l2 LogReg", "l1 LogReg", "DecTree", "RandForest", "SVM", "MLP"], "Column_2": ["Standard", "Enhanced"], "Column_1": ["Enterprise", "Malware"]}, "indexed_table_271.csv": {"Caption": "Table 2: Comparison of NBC, SVM and RFC coupled with various feature sets, in terms of Precision (p), Recall (r) and F-measure (f). wavg denotes weighted average of corresponding measure across three classes.", "Rows_1": ["RFC", "NBC", "SVM"], "Rows_2": ["W", "W_P", "W_P_ET"], "Column_1": ["features", "p_NFS", "p_UFS", "p_CFS", "p_wavg", "r_NFS", "r_UFS", "r_CFS", "r_wavg", "f_NFS", "f_UFS", "f_CFS", "f_wavg"]}, "indexed_table_272.csv": {"Caption": "Table 2: e Accuracy of Diagnosis Prediction Task.", "Rows_1": ["RNN", "Baseline Med2Vec", "RETAIN", "RNNl", "Our RNNc Approach", "Dipole", "Dipolel", "Dipolec"], "Column_2": ["Accuracy"], "Column_1": ["Diabetes", "Medicaid"]}, "indexed_table_73.csv": {"Caption": "Table 3: Accuracy comparison", "Rows_1": ["Training Size", "10K", "100K", "1M"], "Column_2": ["wCNN", "cCNN", "SVM"], "Column_1": ["Validation Data", "Hand labeled Data", "User labeled Data"]}, "indexed_table_126.csv": {"Caption": "Table 2: Comparison on real datasets for kernel SVM. Here we use 32 machines (each machine with 1 thread) for all the distributed solvers (PBM, P-packSGD, PSVM), and 1 machine for the serial solver (DC-SVM).MPI+OpenMP Hybrid Implementation of PBM. The rst column of PBM shows that PBM achieves good test accuracy after 1 iteration, and  For kddb, we use 32 machines and 20 cores in each machine to test Note that \u201c-\u201d indicates the training time is more than 10 hours.", "Rows_1": ["webspam", "covtype", "cifar", "mnist8m", "kddb"], "Column_2": ["time", "acc"], "Column_1": ["PBM", "PBM", "P packSGD", "PSVM p = n0.5", "PSVM p = n0.6", "DC SVM"]}, "indexed_table_132.csv": {"Caption": "Table 2: Average running time \u00b1 the standard derivation (in seconds) of miSVM, miFV, miVLAD and WLRM for training the model on ve groups of GO terms. e last row is the speedup of WLRM with respect to the runtime of the fastest one among other three methods.", "Rows_1": ["miSVM", "miFV", "miVLAD", "WLRM", "Speedup"], "Column_1": ["Group A", "Group B", "Group C", "Group D", "Group E"]}, "indexed_table_435.csv": {"Caption": "Table 3: Experimental results. Unavailable values are denoted as \u201c_\u201d (see text for details).", "Rows_1": ["Amazon", "Wiki", "Delicious", "Wikipedia"], "Rows_2": ["P@1", "P@3", "P@5"], "Column_1": ["AnnexML", "SLEEC", "FastXML", "PfastreXML", "PLT", "PD Sparse", "Most common"]}, "indexed_table_340.csv": {"Caption": "Table 1: Results and statistics for large-scale Multilabel data sets, Ntrain= number of training samples, Ntest= number of testing samples, Ttrain = training time, Ttest = testing time, K = number of classes, D = number of features. P@k = top-k accuracy. DiSMEC and PPDSparse are parallelized with 100 cores. We highlight the best result for each metric, except that for Ttrain we highlight best results among single-core solvers (le four) and parallel solvers. For all experiments, we set a memory limit to be 100G. Experiments that exceeded limits are marked Memory Limit Exceeded (MLE).", "Rows_1": ["Amazon", "WikiLSHTC", "Delicious", "AmazonCat"], "Rows_2": ["Ttrain", "P@1", "P@3", "P@5", "model size", "Ttest Ntest"], "Column_1": ["FastXML", "PfastreXML", "SLEEC", "PDSparse", "DiSMEC", "PPDSparse"]}, "indexed_table_54.csv": {"Caption": "Table 2: Results and statistics for small Multilabel data sets, Ntrain= number of training samples, Ntest= number of testing samples, Ttrain = training time, Ttest = testing time, K = number of classes, D = number of features. P@k = top-k accuracy. DiSMECandPPDSparseareparallelizedwith100cores.Wehighlightthebestresultforeachmetric,exceptthatforTtrain we highlight best results among single-core solvers (le four) and parallel solvers.", "Rows_1": ["Mediamill", "Bibtex", "RCV", "EURLex", "aloi bin", "LSHTC2", "LSHTC6"], "Rows_2": ["P@1", "P@3", "P@5", "accuracy"], "Column_1": ["FastXML", "PfastreXML", "SLEEC", "PDSparse", "DiSMEC", "PPDSparse"]}, "indexed_table_427.csv": {"Caption": "Table 2: e performance of di erent methods. \u2018P\u2019 is precision, \u2018R\u2019 is pseudo recall; and \u2018F1\u2019 is pseudo F1 score.", "Rows_1": ["EvenTweet", "GeoBurst", "GeoBurst+"], "Column_2": ["P", "R", "F1"], "Column_1": ["LA", "NY"]}, "indexed_table_286.csv": {"Caption": "Table 4: Prediction performance using di erent views of the metadata.", "Rows_1": ["DMVM w/o Alph", "DMVM w/o Spec", "DMVM w/o Accel", "DMVM w/ all", "DFM w/ Alph", "DFM w/ Spec", "DFM w/ Accel", "DFM w/ all"], "Column_2": ["Accuracy", "F score", "RMSE"], "Column_1": ["Classification", "Regression"]}, "indexed_table_265.csv": {"Caption": "Table 1: Performance comparison between di erent multi-class classi cation algorithms on Caltech 101 Sihouettes and MIT Indoor 67 datasets. Top-k accuracy is used as an evaluation metric. A larger value indicates a better performance.", "Rows_1": ["Top 1", "Top 2", "Top 5", "SVMOVA", "TopPush", "top 1 SVM", "top 2 SVM", "top 3 SVM", "top 4 SVM", "top 5 SVM", "top 10 SVM", "top 20 SVM", "top 1 SVM", "top 2 SVM", "top 3 SVM", "top 4 SVM", "top 5 SVM", "top 10 SVM", "top 20 SVM", "rtop 1 SVM", "rtop 2 SVM", "rtop 3 SVM", "rtop 4 SVM", "rtop 5 SVM", "rtop 10 SVM", "rtop 20 SVM"], "Column_2": ["Top 1 Acc", "Top 2 Acc", "Top 3 Acc", "Top 4 Acc", "Top 5 Acc", "Top 10 Ac"], "Column_1": ["Caltech 101 Sihouettes", "MIT Indoor 67"]}, "indexed_table_263.csv": {"Caption": "Table 2: Performance comparison between di erent multi-class classi cation algorithms on UCF 101 and FCVID datasets. Top-k accuracy is used as an evaluation metric. A larger value indicates a better performance.", "Rows_1": ["Top 1", "Top 2", "Top 5", "SVMOVA", "TopPush", "top 1 SVM", "top 2 SVM", "top 3 SVM", "top 4 SVM", "top 5 SVM", "top 10 SVM", "top 20 SVM", "top 1 SVM", "top 2 SVM", "top 3 SVM", "top 4 SVM", "top 5 SVM", "top 10 SVM", "top 20 SVM", "rtop 1 SVM", "rtop 2 SVM", "rtop 3 SVM", "rtop 4 SVM", "rtop 5 SVM", "rtop 10 SVM", "rtop 20 SVM"], "Column_2": ["Top 1 Acc", "Top 2 Acc", "Top 3 Acc", "Top 4 Acc", "Top 5 Acc", "Top 10 Ac"], "Column_1": ["UCF101", "FCVID"]}, "indexed_table_353.csv": {"Caption": "Table 2: Results of real-world datasets. Basic statistics of the 16 datasets, results of the PL method and the results of our  method. With respect to KS-Dist error, our model captures the real datasets with much pdfdescribedinPLmethodisf(x)= \u03b1PL_1( x )\u03b1PL.  smaller error than the PL Mehtod. e", "Rows_1": ["Words", "Terrorism", "Species", "Blackouts", "Cities", "Fire", "akes", "Actor", "WeChat", "SMS", "Einstein", "Freud", "Email", "Cascade", "Group Chat", "Wikipedia"], "Column_2": ["N", "Min", "Max", "E", "Std", "PL 1", "KS Dist", "KS Dist"], "Column_1": ["Real World Data Statistics", "PL Method", "Our Method"]}, "indexed_table_384.csv": {"Caption": "Table 4: Multi-label node classi cation performance of the embedding extracted features on the Deezer genre likes datasets. Performance is measured by average F1 score values. Models  were trained on 90% of the data and evaluated on the remaining 10%. Errors in the parentheses correspond to two standard deviations. GEMSEC models consistently have good performance.", "Rows_1": ["DeepWalk", "Smooth DeepWalk", "GEMSEC", "Smooth GEMSEC"], "Column_2": ["Micro", "Macro", "Weighted"], "Column_1": ["Croatia", "Hungary", "Romania"]}, "indexed_table_382.csv": {"Caption": "Table 3: Performance comparisons of di erent methods in sales volume prediction in terms of MAE, RMSE and RMPSE. REST- Ful achieves the best performance in all cases.", "Rows_1": ["SVR", "ARIMA", "ANN", "GRU", "abi GRU", "st GRU", "RESTFul"], "Column_2": ["MAE", "RMSE", "RMSPE"], "Column_1": ["January", "February", "March", "April", "May"]}, "indexed_table_98.csv": {"Caption": "Table 6: Performance investigation with di erent resolution con gurations.", "Rows_1": ["RESTFul 2", "RESTFul 3"], "Column_2": ["MAE", "RMSE", "RMSPE", "AUC", "F1", "Pre", "Rec"], "Column_1": ["Sales Volume", "Email Communication", "Urban Anomaly"]}, "indexed_table_319.csv": {"Caption": "Table 2: Comparison of R2 value for graph convolutional LSTM against other baseline methods.", "Rows_1": ["Previous Time Step", "Gaussian Process", "Graph Convolution", "Local LSTM", "Global LSTM", "GC LSTM", "WGC LSTM"], "Column_2": ["Temperature", "Wind Speed"], "Column_1": ["IGRA", "GSOD"]}, "indexed_table_60.csv": {"Caption": "Table 1: Description of datasets used for classication and clustering experiments", "Rows_1": ["Instances", "Features", "Classes", "Missing"], "Column_2": ["Vowel", "MNIST", "Baseball", "Cancer", "Digits", "IRIS", "Calls"], "Column_1": ["Japanese", "Fashion", "Breast", "Anuran"]}, "indexed_table_348.csv": {"Caption": "Table 3: Classication performance in presence of 10% noise added to columns and rows in each dataset. Best performance is 814 _ighlighted in blue. The numbers in the parenthesis indicate the di erence between the performance with and without noise.", "Rows_1": ["Raw Features", "RandLocal", "Variant I", "Variant II", "RULLS"], "Column_2": ["Japanese Vowel", "Fishion MNIST", "Breast cancer", "Baseball"], "Column_1": ["Add noise to column", "Add noise to rows"]}, "indexed_table_49.csv": {"Caption": "Table 1: Performance comparison for di erent methods. HR is the hitting ratio; R10@50 is the top-50 recall for the top-10 ground truth; \u03b4H 10 is the distortion of average distance on the top 10 results; \u03b4R10 is the distortion of average distance on the top 10 recall in top 50 result.", "Rows_1": ["Geolife", "Porto"], "Rows_2": ["AP", "Siamese", "Siamese", "NT No WS", "NT No WS", "NT No SAM", "NT No SAM", "NeuTraj", "NeuTraj"], "Column_2": ["HR@10", "HR@50", "R10@50"], "Column_1": ["Fr chet", "Hausdorff", "DTW"]}, "indexed_table_64.csv": {"Caption": "Table 1: Performance comparison (mean squared error) on 24 benchmark datasets. The best performance is in boldface. _DC , _T N , _DA are the relative improvements (%) of MPCN over DeepCoNN (D-CON), TransNet (T-NET) and D-ATT respec- tively. MPCN achieves state-of-the-art performance, outperforming all existing methods on 24 benchmark datasets.", "Rows_1": ["Yelp17", "Instant Video", "Instruments", "Digital Music", "Baby", "Patio / Lawn", "Gourmet Food", "Automotive", "Pet Supplies", "Office Products", "Android Apps", "Beauty", "Tools / Home", "Video Games", "Toys / Games", "Health", "CellPhone", "Sports / Outdoors", "Kindle Store", "Home / Care", "Clothing", "CDs / Vinyl", "Movies / TV", "Electronics"], "Column_2": ["MF", "FM", "MLP", "NEUMF", "D CON", "TNET", "D ATT", "MPCN", "C", "N", "A"], "Column_1": ["Interaction based", "Review based", "Improvement"]}, "indexed_table_192.csv": {"Caption": "Table 7: Performance of MFRL and Mix-MFRL in 5-fold cross validation.", "Rows_1": ["MFRL", "Mix MFRL"], "Column_2": ["MAE", "RMSE", "NDCG@1", "NDCG@3", "NDCG@5"], "Column_1": ["K=5", "K=20"]}, "indexed_table_3.csv": {"Caption": "None", "Rows_1": ["Ground truth Modularity", "optimization", "BigClam", "CESNA", "LFA", "FLFA"], "Column_2": ["2010", "2011"], "Column_1": ["culture show", "culture how"]}, "indexed_table_152.csv": {"Caption": "Table 4: The experimental results of all test methods on SCHOOL Data. The evaluation criteria are nMSE and aMSE with standard deviation. The optimal parameters of alternative methods are selected by 5-fold cross-validation.", "Rows_1": ["nMSE", "aMSE"], "Rows_2": ["Training ratio", "Training10%", "Training20%", "Training30%"], "Column_1": ["Future Based", "Low Rank Based"], "Column_2": ["Lasso", "Calibrated MTL", "NN MTL", "Capped MTL", "Robust MTL", "NC CMTL"]}, "indexed_table_154.csv": {"Caption": "Table 6: The time consuming of four low rank based algorithms on benchmark datasets. The training da- ta are 20% and 300 respectively.", "Rows_1": ["SCHOOL", "SARCOS"], "Column_1": ["NN MTL", "Capped MTL", "Robust MTL", "NC CMTL"]}, "indexed_table_174.csv": {"Caption": "Table 4: Results of different models as second movers. The budget of seed set k = 10, and the maximum inference round r = 5. The results are reported as \u201caverage opinions + (rank)\".", "Rows_1": ["Random", "IM", "OM", "ICIM", "ICOM"], "Column_2": ["Item", "C1", "C2", "C3", "X1", "X2", "X3", "T1", "T2", "T3", "Rank"], "Column_1": ["Ciao", "Flixster", "Filmtrust", "Avg"]}, "indexed_table_13.csv": {"Caption": "Table 3: Performance of the various models on the multi-section CNN/Daily Mail test set. For all but the 1X task where Lead is best, LeadR consistently outperforms the baselines on ROUGE-1, ROUGE-2 and ROUGE-L.", "Rows_1": ["Lead", "Leadmulti", "Luhn", "SumBasic", "LSA", "TextRank", "LexRank", "KLSum", "LeadR", "LeadR 1", "LeadR 0", "LeadRav"], "Column_2": ["R 1", "R 2", "R L"], "Column_1": ["1X concat", "2X concat", "3X concat", "4X concat", "5X concat"]}, "indexed_table_255.csv": {"Caption": "Table 3: Result comparison under di erent amounts of labels. Highlighted results are signi cant (0.01 level paired t-test).", "Rows_1": ["NDCG@10"], "Rows_2": ["SRW", "DeepWalk", "ProxEmbed", "Metapath2vec", "DAG LSTM", "IPE", "IPE", "IPE"], "Column_1": ["LinkedIn", "Facebook", "DBLP", "Taobao"], "Column_2": ["schoolmate", "colleague", "family", "advisor", "advisee", "oneID"]}, "indexed_table_375.csv": {"Caption": "Table 3: Performance measures (%) of models over cross-validation dataset and test dataset", "Rows_1": ["glm", "glmnet", "rpart", "ctree", "rf", "svmLinear", "svmLinear2", "svmLinear3", "svmRadial", "nnet"], "Column_2": ["Accuracy", "Sensitivity", "Specificity", "F1 Scor"], "Column_1": ["Cross validation dataset", "Test dataset"]}, "indexed_table_85.csv": {"Caption": "Table 5: Performance of rating prediction for all compared algorithms w.r.t. different time windows.", "Rows_1": ["PMF", "BPMF", "TDSSM", "RRN", "NCF", "NTFdot", "NTF", "NTF", "NTF"], "Column_2": ["RMSE", "MAE"], "Column_1": ["2004 Jan", "2004 Mar", "2004 May", "2004 Jul", "2004 Sep", "2004 Nov"]}, "indexed_table_285.csv": {"Caption": "Table 3: Event Extraction performance. Comparison based on micro-average Precision, Recall and F-1 Score w.r.t to baseline methods.", "Rows_1": ["SVM", "H RNN", "H CNN", "MI RNN", "MI CNN", "MIMT RNN"], "Column_2": ["Precision", "Recall", "F1"], "Column_1": ["Cyber Attack", "Civil Unrest"]}, "indexed_table_140.csv": {"Caption": "Table 7: Performance Comparison models with and without auxiliary task", "Rows_1": ["Attacker", "Target", "Time", "None", "Avg Total", "Civil Unrest"], "Column_2": ["Precision", "Recall", "F1 Score"], "Column_1": ["Without Auxiliary Task", "With Auxiliary Task"]}, "indexed_table_93.csv": {"Caption": "Table 2: Performance comparison results of various methods on the segment recovery task.", "Rows_1": ["LSH MH", "LSH RP", "LSH BRP", "BAE FFN", "BAE Seq", "BAAE", "BAAE S", "BAAE SD"], "Column_2": ["Precision", "Recall", "F1"], "Column_1": ["m=1", "m=2", "m=5"]}, "indexed_table_103.csv": {"Caption": "Table 3: AP vs EAP on real world datasets", "Rows_1": ["Optdigits", "MNIST", "Proteins"], "Column_2": ["Sn", "PPV", "Accuracy"], "Column_1": ["AP", "EAP"]}, "indexed_table_165.csv": {"Caption": "Table 2: Parzen window log-likelihood estimates for di erent types of proteins.", "Rows_1": ["actinin", "tubulin", "actin", "Desmoplakin", "Fibrillarin", "LaminB1", "Myosin IIB", "Sec61", "Tom20", "ZO1"], "Column_1": ["Self Gated", "Encoder Gated", "Label Gated", "Copy Connection", "AAE Based"]}, "indexed_table_223.csv": {"Caption": "Table 1: Performance of HUCE compared to baselines (%)", "Rows_1": ["uDeepWalk", "LDA", "IAD", "hDeepWalk", "metapath2vec", "HUCE", "HUCE U", "HUCE M", "HUCE UM", "HUCE"], "Column_2": [], "Column_1": ["Accuracy", "F1 score"]}, "indexed_table_36.csv": {"Caption": "Table 1: Performance of HUCE compared to baselines (%)", "Rows_1": ["HUCE U", "HUCE M", "HUCE UM", "HUCE"], "Column_1": ["Accuracy", "F1 score", "Accuracy", "F1 score"]}, "indexed_table_72.csv": {"Caption": "Table 1: Performance Comparison", "Rows_1": ["TruthFinder", "Estimates", "AverageLog", "Investment", "PooledInv", "Median", "Mean", "GTM", "GTM+ours", "DynaTD+All", "EvolvT", "EvolvT", "EvolvT", "EvolvT"], "Column_2": ["MAE", "RMSE", "Time"], "Column_1": ["Stock", "Flight", "Weather", "Pedestrian"]}, "indexed_table_87.csv": {"Caption": "Table 2: Multivariate time series retrieval performance (MAP) on EEG Eye State, PAMAP2, and SHL when v = 32, 64, and 128. The best MAP is displayed in bold-face type.", "Rows_1": ["LSH", "ITQ", "HDML", "TopRSBC", "CNNs+Pairwise loss", "LSTM+Triplet loss", "LSTM+r th root ranking loss", "Deep r RSJBE"], "Column_2": ["v=32", "v=64", "v=12", "v=3", "v=6", "v=12"], "Column_1": ["EEG Eye State", "PAMAP2", "SHL"]}, "indexed_table_387.csv": {"Caption": "Table 2: Experimental results on the MNIST data set when the number of hashing bits is 16. Training and test times are in seconds. The best results are highlighted in bold face.", "Rows_1": ["R2SDH", "SDH", "SDHR", "BRE", "KSH", "SSH", "CCA ITQ", "FastHash", "PCA ITQ", "AGH", "IMH"], "Column_1": ["precision@r=2", "recall@r=2", "F-measure@r=2", "MAP", "accuracy", "training time", "test time"]}, "indexed_table_77.csv": {"Caption": "Table 4: Experimental results on the NUS-WIDE database when the number of hashing bits is 64. Training and test times are in seconds. The best results are highlighted in bold face.", "Rows_1": ["R2SDH", "SDH", "BRE", "KSH", "SSH", "CCA ITQ", "FastHash", "PCA ITQ", "AGH", "IMH"], "Column_1": ["precision@r=2", "recall@r=2", "F-measure@r=2", "MAP", "training time", "testing time"]}, "indexed_table_241.csv": {"Caption": "Table 1: Statistics of the datasets in Gold Data.", "Rows_1": ["Camera", "DVD Player", "MP3", "Laptop"], "Column_2": ["Size", "Train", "Test"], "Column_1": ["Positive", "Neutral", "Negative"]}, "indexed_table_178.csv": {"Caption": "Table 2: Binary classi cation results on all three datasets of Gold Data.", "Rows_1": ["Knowledge Group Models", "AE LSTM LSTMs", "ATAE LSTM", "MN", "No Knowledge MNL", "MNs MNA", "AMN", "AMNM", "RKMN", "KDMN", "NLKs LexMN", "UKMN Knowledge", "UDKMN", "ASA", "LMNs CSE", "JOINT"], "Column_2": ["Mac", "Neg", "Pos"], "Column_1": ["Camera", "DVD Player", "MP3"]}, "indexed_table_378.csv": {"Caption": "Table 5: The detection accuracy of the evaluated methods when applied to detect the DMVs on di erent data sets.", "Rows_1": ["UCI ML Repo 4", "data gov 10", "data gov uk 3", "MIT DWH 9", "mass gov 6"], "Column_2": ["Precision", "Recall"], "Column_1": ["DiMaC dBoost", "FAHES SynPat", "OD"]}, "indexed_table_426.csv": {"Caption": "Table 4: Node Classi cation Results (10% train data)", "Rows_1": ["Cora", "Citeseer", "Wiki", "Washington", "Wisconsin", "Texas", "Cornell", "PPI", "Blogcatalog", "Rank", "Score"], "Column_2": ["SS NMF", "MMDW", "MFDWL", "MF Planetoid", "MNMFL", "MNMF", "MFDW", "DW", "N2V"], "Column_1": ["Proposed", "SoA Proposed Baseline Variants", "SoA", "SoA SoA"]}, "indexed_table_448.csv": {"Caption": "Table 6: Clustering results (50% train data)", "Rows_1": ["Cora", "Citeseer", "Wiki", "Washington", "Wisconsin", "Texas", "Cornell", "Rank", "Score"], "Column_2": ["SS NMF", "MMDW", "MFDWL", "MF Planetoid", "MNMFL", "MNMF", "MFDW", "DW", "N2V"], "Column_1": ["Proposed", "SoA Proposed Baseline Variants", "SoA", "SoA SoA"]}, "indexed_table_305.csv": {"Caption": "Table 3: Accuracy evaluation on Yahoo. \u2018ND\u2019 is short for NDCG.", "Rows_1": ["Most Popular", "SGM_1", "SGM_4", "SGM_16", "SGM_64", "SGM_256", "PRFM", "\u6d63_M", "A SGNS", "IRGAN", "eVGM_", "eVGM", "Improv"], "Column_1": ["ND@10", "ND@50", "MRR@10", "MRR@50"]}, "indexed_table_70.csv": {"Caption": "Table 4: Accuracy evaluation on Last.fm. The results of SGM, PRFM, \u03bbFM and eVGM are reported with all contextual features.", "Rows_1": ["Most Popular", "SGM_1", "SGM_4", "SGM_16", "SGM_64", "PRFM", "\u6d63_M", "A SGNS", "IRGAN", "eVGM_", "eVGM", "Improv"], "Column_1": ["ND@10", "ND@50", "MRR@10", "MRR@50"]}, "indexed_table_412.csv": {"Caption": "Table 2: The overall IRMiner mining results compared with AMIE, AMIE + and OP", "Rows_1": ["YAGO 2", "YAGO 2s", "DBPedia 3.8", "Wikidata"], "Column_2": ["Rules", "Rules Pruned", "Cleaned Expanded"], "Column_1": ["AMIE", "AMIE+", "OP", "IRMiner"]}, "indexed_table_215.csv": {"Caption": "Table 2: Spatiotemporal prediction performance for all the methods in all the datasets", "Rows_1": ["LR", "LASSO", "SAR", "SAR L1", "GL", "SADL I", "SADL II"], "Column_2": ["RMSE", "MAE"], "Column_1": ["Flu outbreak", "Traffic volume", "Water quality"]}, "indexed_table_34.csv": {"Caption": "Table 5: Performance of the Classi ers", "Rows_1": ["Accuracy", "Instead of updating the ATR scores of all the nodes at everyNormal Precision", "Normal Recall", "Normal F1", "Spam Precision", "Spam Recall", "Spam F1", "Avg Precision", "Avg Recall", "Avg F1"], "Column_2": ["SVM", "KNN", "RF"], "Column_1": ["W1", "W2"]}, "indexed_table_273.csv": {"Caption": "Table 2: Comparison between our proposed approaches.", "Rows_1": ["groceries", "ml10m", "jester", "flixster", "netflix"], "Column_2": ["Cls", "f", "HR", "f"], "Column_1": ["LSVD", "rLSVD", "vLSVD", "GLSVD", "rGLSVD", "vGLSVD"]}, "indexed_table_217.csv": {"Caption": "Table 3: Comparison with competing latent space approaches.", "Rows_1": ["groceries", "ml10m", "jester", "flixster", "netflix"], "Column_2": ["rank", "HR", "factors", "lrnrate re", "Cls"], "Column_1": ["LLORMA", "PureSVD", "BPRMF", "rGLSVD"]}, "indexed_table_90.csv": {"Caption": "Table 1: Test Rank-Invest Models in ree Years", "Rows_1": ["AR model", "MD model", "SR model"], "Column_2": ["ER", "AR", "MD", "SR", "WR"], "Column_1": ["Year", "2014", "2015", "2016"]}, "indexed_table_83.csv": {"Caption": "Table 3: The effect of different input information for attention model- ing, with \u201cBase\u201d denotes the base embedding, \u201cAuxi\u201d denotes the auxil- iary embedding and \u201cVisual\u201d denotes the visual feature.", "Rows_1": ["Model", "HASC"], "Rows_2": ["Base", "Base+Auxi", "Base+Auxi+Visual"], "Column_1": ["F_S", "F_L"], "Column_2": ["HR", "NDCG"]}, "indexed_table_20.csv": {"Caption": "Table 2: results on warm-start emotion prediction task", "Rows_1": ["PROMO", "PROMO_NT", "RESCAL", "CP", "MultiMF", "eToT"], "Column_2": ["CNN", "foxnews", "COMBINE"], "Column_1": ["LogPPL", "AUCPR"]}, "indexed_table_425.csv": {"Caption": "Table 6: Leave-one-out classi cation scores showing how accuracies, ADR F-scores, ADR precisions and ADR recalls are affected after removing one feature", "Rows_1": ["All", "Word Cluster", "Word Order", "Word Length", "Subjectivity", "Pos WordNet", "Pos Opinion", "Neg WordNet", "Neg Opinion", "More Good", "More Bad", "Less Good", "Less Bad", "ADR"], "Column_2": ["Mean", "SD"], "Column_1": ["ADR F score", "ADR Precision", "ADR Recall"]}, "indexed_table_67.csv": {"Caption": "Table 2: Summary of datasets and their cluster proles.", "Rows_1": ["karate", "dolphins", "us football", "polblogs", "ca hepth", "ca condmat", "email enron", "youtube", "dblp"], "Column_2": ["Node", "Edg", "RankCom"], "Column_1": ["Characteristics", "Cluster", "Max members"]}, "indexed_table_56.csv": {"Caption": "Table 3: Performance on Clustering evaluated by Modularity and Permanence(r ank )", "Rows_1": ["karate", "dolphins", "us football", "ca hepth", "condmat", "enron email", "polblogs"], "Column_2": ["FS GRL", "node2vec", "LINE", "Deepwalk"], "Column_1": ["Modularity", "Permanence"]}, "indexed_table_175.csv": {"Caption": "Table 2: Accuracy on the real-world datasets with di erent label values (in percent)", "Rows_1": ["D2", "D3", "D4", "D5"], "Column_1": ["MV", "iBCC", "DS", "SpectralDS", "MCMLI", "MCMLD"]}, "indexed_table_1.csv": {"Caption": "Table 2: Performance measured by MSE", "Rows_1": ["BASE", "VAR", "Social Sensor", "Information Cascades", "Social Actuator"], "Column_2": ["volume_1", "volume_3", "volume_5", "volume_30"], "Column_1": ["Bitcoin", "Etheruem", "Ripple"]}, "indexed_table_439.csv": {"Caption": "Table 1. Performance in terms of F1 score, accuracy (ACC) and mean rank error (MRE) of LTGL with respect to TVGL, LVGLASSO and GL. LTGL and TVGL are employed with both l2 and l1 penalties, to show how the prior on the evolution of the network a ects the outcome.", "Rows_1": ["LTGL", "LTGL", "TVGL", "TVGL", "LVGLASSO", "GL"], "Column_2": ["F1", "ACC", "MRE"], "Column_1": ["l2", "l1"]}, "indexed_table_220.csv": {"Caption": "Table 2: Results per dataset. Area Under the ROC Curve (AUC) (weighted AUC for multiclass datasets), average number of rules |R| (including the default rule), and average number of items per rule (|I|). Note that SBRL cannot do multiclass classi cation, hence the _s.", "Rows_1": ["adult", "breast", "chessbig", "cylBands", "heart", "hepatitis", "horsecolic", "ionosphere", "iris", "led7", "mushroom", "pageblocks", "pendigits", "pima", "tictactoe", "waveform", "wine"], "Column_2": ["AUC", "|R|", "|I|"], "Column_1": ["SBRL", "CART", "C5 0", "Ripper", "SVM"]}, "indexed_table_296.csv": {"Caption": "Table 2: The Context coherence . This shows the coherence of a concepts should decrease as the depth of the tree in- crease. For all datasets, CCM and SSA show as depth of the tree increases, the concepts scores decrease. However, in rCRP and hPAM the pattern is di erent.", "Rows_1": ["CCM", "rCRP", "hPAM", "SSA"], "Column_2": ["Level1", "Level2", "Level3", "Level4"], "Column_1": ["SmartPhone", "DBLP"]}, "indexed_table_151.csv": {"Caption": "Table 1: Classi cation performance of our DualSVRG(s) and the baselines in the batch mode. The error is reported in percent 770  714 (%), the training time is in second. The best and runner-up performances are in bold and italic-bold respectively.", "Rows_1": ["LIBSVM", "LLSVM", "BSGD M", "BSGD R", "FOGD", "NOGD", "DualSGD Hinge", "DualSVRG H B", "DualSVRG H C", "DualSVRG H AM", "DualSVRG L B", "DualSVRG L C", "DualSVRG L AM"], "Column_2": ["Error", "Time"], "Column_1": ["w8a", "cod rna", "covtype", "airlines"]}, "indexed_table_338.csv": {"Caption": "Table 2: Performance of the baseline methods and the interdependence model with di erent prediction methods. A number in a bracket indicates the ranking of each method. Winners are bold-faced. ES and ES+ outperform the baseline methods in most cases when the performance is evaluated by 0/1 loss. Our methods achieves higher accuracies and F1 scores than the baseline methods in Birds, Enron, and Medical.", "Rows_1": ["Scene", "Emotions", "Yeast", "Flags", "Birds", "Enron", "Medical", "Avg rank", "Avg Rank", "Dataset"], "Column_2": ["BR", "LP", "MS", "SM", "CC", "Gibbs", "FPI", "ES", "ES+"], "Column_1": ["Baseline methods", "Interdependence model"]}, "indexed_table_352.csv": {"Caption": "Table 3: Performance of the baseline methods and the interdependence model with di erent prediction methods. A number in a bracket indicates the ranking of each method. Winners are bold-faced.", "Rows_1": ["Scene", "Emotions", "Yeast", "Flags", "Birds", "Enron", "Medical", "Avg Rank"], "Column_2": ["BR", "LP", "MS", "SM", "CC", "Gibbs", "FPI", "ES", "ES+"], "Column_1": ["Baseline", "Interdependence model"]}, "indexed_table_301.csv": {"Caption": "Table 2: Performance while shifting parameter \u03b3", "Rows_1": ["Domain", "University", "Politician", "City", "Song", "Sportsman", "Actor/Singer"], "Column_2": ["Triple", "Precision"], "Column_1": []}, "indexed_table_213.csv": {"Caption": "Table 6: Rankings w.r.t ARI, NMI and accuracy", "Rows_1": ["SPK HAC 4", "SPK+KKM 8", "Kernel for SVM Protein Classification  In Pacific Symposium on Biocomputing SPK+KSC 6", "CPR HAC 2", "CPR KKM 5", "CPR KSC 1", "CPI HAC 3", "CPI KKM 9", "CPI KSC 7"], "Column_2": [], "Column_1": ["NMI", "Accuracy", "Average"]}, "indexed_table_181.csv": {"Caption": "Table 4: Classi cation error rates on KARD data set.  Table 5: Classi cation error rates on KARD data set with appended noise activities.", "Rows_1": ["DG PG", "DG Dir", "Proj", "ScProj", "AffProj", "AffScProj", "BC", "PML", "GDA", "GGDA", "EXP"], "Column_1": ["1A", "1B", "1C", "2A", "2B", "2C", "3A", "3B", "3C"]}, "indexed_table_240.csv": {"Caption": "Table 2: Comparisons among 8 di erent variants of RSE in terms of classi cation accuracy.", "Rows_1": ["ding protein", "fold", "superfamily", "splice", "dna3 class1", "dna3 class2", "dna3 class3", "mnist str4", "mnist str8"], "Column_2": ["Accu"], "Column_1": ["RSE", "RSE", "RSE", "RSE", "RSE", "RSE", "RSE"]}, "indexed_table_306.csv": {"Caption": "Table 3: The performance metrics of the compared algorithms. The boldface font denotes the winner in that row, and the second number in a cell shows the relative performance gain of SoEXBMF compared to that method.", "Rows_1": ["Ciao", "Epinions", "LastFM"], "Rows_2": ["Pre@5", "Impv"], "Column_1": ["SPF", "WMF", "EXMF", "SERec Bo", "EXBMF", "SERec Bo B", "SoEXBMF CH", "SoEXBMF C", "SoEXBMF K", "SoEXBMF"]}, "indexed_table_163.csv": {"Caption": "Table 4: Performance comparison among di erent algorithms on four datasets.", "Rows_1": ["LR", "DT", "RF", "SVM", "KNN", "NB", "CF", "DeepWalk", "LINE", "node2vec", "struc2vec", "TADW", "CANE", "LRCNN", "MACNN", "HIEPT 1D", "HIEPT"], "Column_2": ["macro P", "macro F1", "Accuracy"], "Column_1": ["Twitter", "WeiBo"]}, "indexed_table_35.csv": {"Caption": "Table 3: Performance comparison among di erent algorithms for RT-TSCI on four datasets.", "Rows_1": ["Co Visit", "MF", "Decision", "SVM[32", "RandomForest", "MLP", "TULER", "DeepMIML", "DeepTSCI LSTM", "Bi DeepTSCI", "DeepTSCI AE", "DeepTSCI VAE"], "Column_2": ["macro R", "macro F1", "Accuracy"], "Column_1": ["Brightkite", "Gowalla", "Tokyo", "New York"]}, "indexed_table_420.csv": {"Caption": "Table 3: Regression performance on di erent tasks, shown in \u201cmean (\u00b1 std)\u201d format, evaluated using RMSE. There are two 755 sets of experiments, one with both bipolar and control subjects (w/ ctrl), the other with only bipolar subjects (w/o ctrl).", "Rows_1": ["RNN", "CNN", "CNNRNN", "CNNRNN Cr", "CNNRNN PsCr", "CNNRNN fillna", "765CNNRNN dropna", "dpMood fillna", "dpMood dropna"], "Column_1": ["HDRS", "ctrl", "YMRS"]}, "indexed_table_438.csv": {"Caption": "Table 4: Mean of link prediction results with standard error in brackets.", "Rows_1": ["Snapchat", "YouTube", "Twitter"], "Rows_2": ["ROC AUC", "PRC AUC", "ROC AUC", "PRC AUC"], "Column_1": ["single viewDataset Metric", "mvn2vec reg", "independent", "one space", "view merging", "mvn2vec con"]}, "indexed_table_17.csv": {"Caption": "Table 1: Abusive sellers: un-Supervised and semi-supervised results - relative performance.", "Rows_1": ["Un Supervised", "Semi Supervised"], "Rows_2": ["M Zoom", "BPTF", "BNBCP", "Logistic CP"], "Column_1": ["Precision", "Recall", "AUC"]}, "indexed_table_341.csv": {"Caption": "Table 1: Statistics of the three datasets. The rst row of each dataset corresponds to the number of users, items and inter- actions. The last column reports the selected meta-paths in each dataset.", "Rows_1": ["UMGMMovielens", "UAUALastFM", "Yelp"], "Rows_2": ["User Movie", "User User", "Movie Movie", "Movie Genre", "User Artist", "Artist Artist User User", "Artist Tag", "User Business User User", "Business City", "Business Category"], "Column_1": ["#A", "#B", "#AB"]}, "indexed_table_102.csv": {"Caption": "Table 2. The comparison of evaluation indicators", "Rows_1": ["Precision", "Recall", "Accuracy", "F1", "AUC"], "Column_1": ["DT", "NB", "SVM", "LR", "RF", "GBDT", "Kinship GL"]}, "indexed_table_304.csv": {"Caption": "Table 1: Comparison results on RottenTomatoes and Idebate", "Rows_1": ["Lead 3", "TextRank", "LexRank", "Sa Lead 3", "Sim Lead 3", "Sim Sa Lead 3", "NN ABS", "doc Distraction", "Att lead n", "Doc h", "Doc h att", "Doc hh att", "Our method"], "Column_2": ["Rouge 1", "Rouge 2", "Rouge L", "METEOR", "Rouge"], "Column_1": ["RottenTomatoes", "Idebate"]}, "indexed_table_274.csv": {"Caption": "None", "Rows_1": ["LE", "AV raw", "AV maj", "AV prob", "IAV raw", "IAV maj", "IAV prob", "Empirical", "Empirical P", "IAV HR"], "Column_2": ["PRE", "REC", "F1"], "Column_1": ["Prior", "Posterior"]}, "indexed_table_446.csv": {"Caption": "Table 1: Average and standard errors of AUCs over all test time units with N labeled samples and M xed unlabeled (test) samples. Values in boldface are statistically better than others (in paired t-test, p=0.05)", "Rows_1": ["Synth1", "Synth2", "SPAM2", "ELEC2", "ONP", "BLOG"], "Column_1": ["N M", "Proposed", "LR", "SSLR all", "TLLR all", "SSLR t", "TLLR t", "INCLR", "GPFC"]}, "indexed_table_99.csv": {"Caption": "Table 1: Statistics of the network data sets.", "Rows_1": ["Collaboration", "NLP", "Social"], "Rows_2": ["DBLP", "SO", "Citeseer", "Cora", "Pubmed", "Epinion"], "Column_1": ["Classes", "Smallest", "Nodes", "Edges"]}, "indexed_table_315.csv": {"Caption": "Table 3: MSE as a function of k and s for kNN and kCNN. 100 training instances and p = 2 were used. The results were the averages of 10 replicates.", "Rows_1": ["0.1", "0.5", "1", "1.5", "2", "p", "5", "10", "30", "50"], "Column_2": ["kNN", "kCNN"], "Column_1": ["k=1", "k=5", "k=10", "k=20"]}, "indexed_table_310.csv": {"Caption": "Table 4: Average F1 on Text. The first row presents the aug- mented class of the twelve classes.", "Rows_1": ["miSVM based", "MIEN metric", "Method"], "Column_1": ["AL", "CLST", "DPL", "MTCL", "MIL", "MLL"]}, "indexed_table_324.csv": {"Caption": "Table 7: Classification accuracy (mean\u00b1std.) on five MIL benchmark data sets. Note that, MIEN-metric is still allowed to return augmented class predictions in this traditional binary classification MIL setting. The prediction ratio is also reported below. (\u201c\u2193\u201d: The lower, the better.)", "Rows_1": ["miSVM", "MIBoosting", "miFV", "miGraph", "MIEN metric", "Prediction ratio of novel class bags"], "Column_1": ["Musk1", "Musk2", "Elephant", "Fox"]}, "indexed_table_444.csv": {"Caption": "Table 3: Classi cation performance comparison on Economics in terms of Micro-F1 and Macro-F1.", "Rows_1": ["BOW", "SG", "CBOW", "LINE", "Doc2Vec", "DGTF w", "DGTF d"], "Column_2": ["Micro-F1", "Macro-F1"], "Column_1": ["1%", "5%", "10%", "15%", "20%"]}, "indexed_table_281.csv": {"Caption": "Table 2: The accuracy of the instance-level mechanism (InML) under di erent training dataset size", "Rows_1": ["InML", "Cosine", "Euc", "GMML", "ITML", "LMNN", "LowRank", "R2ML"], "Column_2": ["Concrete", "Housing", "Energy", "Cancer", "Stock", "Machine", "Movie", "Music"], "Column_1": ["Regression Dataset", "Ordinal Datasets", "Crowdsourced Datasets"]}, "indexed_table_111.csv": {"Caption": "Table 3: The accuracy of the instance-level mechanism (InML) under di erent noise levels", "Rows_1": ["InML", "Cosine", "Euc", "GMML", "ITML", "LMNN", "LowRank", "R2ML"], "Column_2": ["Concrete", "Stock", "Machine"], "Column_1": ["Weak noise", "Moderate noise", "Strong noise"]}, "indexed_table_298.csv": {"Caption": "None", "Rows_1": ["Dunnhumby", "Instacart", "MSR Grocery", "MSR Grocery", "Dataset"], "Rows_2": ["AUC", "Recall@10 Instacart", "Recall@10", "MSR Grocery AUC", "Metric"], "Column_1": ["itemPop overall", "itemPop user wise", "BPR MF", "FPMC", "item2vec original", "item2vec adaLoyal", "prod2vec original", "prod2vec adaLoyal", "metapath2vec original", "metapath2vec adaLoyal"]}, "indexed_table_295.csv": {"Caption": "Table 4: Systematic evaluations of network embedding meth- ods on the multi-label prediction task. Each cell contains the method name and its Micro-F1 and Macro-F1 scores. The pro- posed methods outperform the existing methods.", "Rows_1": ["N1", "N2", "N3"], "Column_2": ["SVD", "LINE", "BPR", "LIWINE", "PAWINE"], "Column_1": ["PPI data"]}, "indexed_table_367.csv": {"Caption": "Table 1: Results on Single Logit classi cation, using the di erent loss functions. In almost all cases, loss functions that are aligned with the Principle of Logit Separation (under the dashed line) yield a relative improvement of 20%-35% in the di erent performance measures, while also yielding a small improvement in classi cation performance.", "Rows_1": ["MNIST", "SVHN", "CIFAR 10", "CIFAR 100", "Imagenet"], "Rows_2": ["CE", "max margin", "self norm", "NCE", "binary CE", "batch CE", "batch max margin", "CE with all logits"], "Column_1": ["AUPRC", "Precision@0.9", "Precision@0.99", "Acc"]}, "indexed_table_430.csv": {"Caption": "Table 2: Performance of the baselines when we restrict their setting to that of GAM where they are given 20 randomly selected partial snapshots of each graph and have to predict by voting. The column \u201cfull\" indicates the performance when the entire graph is seen and \u201cpartial\" shows the performance when only parts of the graph is seen. \u201cDi .\" is the di erence in performance, a \u2193 means that performance deteriorated when only partial information is available and \u2191 shows increase in performance.", "Rows_1": ["Agg Attr", "Agg WL", "GAM"], "Column_2": ["full", "partial", "diff"], "Column_1": ["HIV", "NCI 1", "NCI 33", "NCI 83"]}, "indexed_table_370.csv": {"Caption": "Table 4: Test accuracy on datasets with 100% added noise fea- tures, avg\u2019ed across 10 samples; 15 mins of hyperparameter tuning on T = 32 threads. Symbols (p<0.005) and \u25b3 (p<0.01) denote the cases where PG-learn is signi cantly better than the baseline w.r.t. the paired Wilcoxon signed rank test.", "Rows_1": ["COIL", "USPS", "MNIST", "UMIST", "YALE"], "Column_1": ["PG Lrn", "MinEnt", "Grid", "Randd"]}, "indexed_table_172.csv": {"Caption": "Table 1: Performance Comparison", "Rows_1": ["ECDIRE", "RelClass", "ECTS", "EDSC", "DTEC"], "Column_2": ["Error Rat", "Earliness", "F score"], "Column_1": ["CBF", "TwoLeadECG", "Coffee"]}, "indexed_table_169.csv": {"Caption": "Table 1 Performance comparison of different approaches for traffic speed forecasting. The proposed TCG-LSTM with the RTBL strategy achieves the best performance with all the three metrics. (K=3 and m=3 in the proposed model)", "Rows_1": ["ARIMA", "SVR", "FNN", "LSTM", "G LSTM", "HCG LSTM", "HGC LSTM RTBL"], "Column_2": ["MAE", "STD", "MAPE", "RMSE"], "Column_1": ["Loop Data", "INRIX Data"]}, "indexed_table_361.csv": {"Caption": "Table 1: Yelp network statistics", "Rows_1": ["State", "Complete", "NV", "AZ", "ON", "WI", "EDH"], "Column_2": ["User to Use", "User to Busines", "Business to Attribute", "Business to Categorie", "Business", "Users", "Attributes", "Categories"], "Column_1": ["No of Edges", "No of Nodes"]}, "indexed_table_65.csv": {"Caption": "Table 2: Results of data stream classi cation", "Rows_1": ["Baseline", "KS", "PT", "PTe", "Topline"], "Column_2": ["accuracy", "drift rate"], "Column_1": ["Insects", "Arabic", "Bike", "Posture"]}, "indexed_table_288.csv": {"Caption": "Table 3: Performance comparison between concatenation-based (MGCNN and SLCNN) and matching-based neural network models (MGMNN, MGANN and SLMNN) on Quora-QP corpus.", "Rows_1": ["MGCNN", "MGMNN", "MGANN", "SLCNN ARC I", "SLMNN"], "Column_1": ["PRECISION", "RECALL", "F1 SCORE", "ACCURACY"]}, "indexed_table_267.csv": {"Caption": "Table 2: Test performance of di erent methods. Best results are in bold face. (Improve. denotes the improvement of Tran-sCF over the best competitor, which is CML.)", "Rows_1": ["Delicious", "Ciao", "Bockcrossing", "Amazon C&A"], "Rows_2": ["HR@10", "HR@20", "NDCG@10", "NDCG@20", "MRR@10", "MRR@20"], "Column_1": ["Metrics", "WMF", "BPR", "CDAE", "NeuMF", "CML", "TransCFalt", "TransCF", "Improve"]}, "indexed_table_23.csv": {"Caption": "Table 5: Performance comparison of di erent methods in user geo-location inference. (\u201cAcc.\u201d means Accuracy (%), and \u201cMED\u201d means Mean Error Distance (km).)", "Rows_1": ["Content", "Logistic Regression", "SVM", "FindMe", "GCN", "SSFGM", "SSFGM", "SSFGM", "SSFGM"], "Column_2": ["Acc", "Acc@3", "MED"], "Column_1": ["Twitter", "Twitter", "Weibo"]}, "indexed_table_108.csv": {"Caption": "Table 3: F1-score on Cora dataset, where the percentage of labeled nodes varies from 10% to 50%.", "Rows_1": ["DeepWalk", "MMDW", "Text Features", "PLSA", "Na_ve Combination", "NetPLSA", "TADW", "STNE"], "Column_1": ["Training 10%", "Training 20%", "Training 30%", "Training 40%", "Training 50%"]}, "indexed_table_414.csv": {"Caption": "Table 4: F1-score on Citeseer dataset, where the percentage of labeled nodes varies from 10% to 50%.", "Rows_1": ["DeepWalk", "MMDW", "Text Features", "PLSA", "Na_ve Combination", "NetPLSA", "TADW", "STNE"], "Column_1": ["Training 10%", "Training 20%", "Training 30%", "Training 40%", "Training 50%"]}, "indexed_table_397.csv": {"Caption": "Table 5: F1-score on Wiki dataset, where the percentage of labeled nodes varies from 10% to 50%.", "Rows_1": ["DeepWalk", "MMDW", "Text Features", "PLSA", "Na_ve Combination", "NetPLSA", "TADW", "STNE"], "Column_1": ["Training 10%", "Training 20%", "Training 30%", "Training 40%", "Training 50%"]}, "indexed_table_155.csv": {"Caption": "Table 1: AUC for di erent network embedding approaches on 3 public data sets.", "Rows_1": [], "Rows_2": ["LINE 1st", "LINE 2nd", "SDNE", "DeepWalk", "Node2Vec", "RANE"], "Column_1": ["Facebook", "arXiv", "PPI"]}, "indexed_table_344.csv": {"Caption": "Table 4: SVM results: Comparing RBF kernel, GMML, SimpleMKL, Laplacian kernel and Isolation Kernel. GMML and Sim- pleMKL have \u201cout of memory\u201d error on 2 datasets; SimpleMKL cannot return any result within 24 hours on 5 datasets. The rst and second subsets consists of 2-class datasets having less than and more than 5000 points, respectively; the third subset consists of multi-class datasets.", "Rows_1": ["GPS", "Heart", "Breast", "Ionosphere", "Vote", "ILPD", "WBC", "Austra", "German", "Parkinson", "QSAR", "Messidor", "Spam", "Wilt", "Mushrooms", "Phishing", "a8a", "IJCNN", "Urban", "Air", "Forest", "Vowel", "Corel"], "Column_1": ["RBF", "GMML", "SimpleMKL", "Laplacian", "Isolation"]}, "indexed_table_53.csv": {"Caption": "Table 2: Results form the real datasets for the RMSE and ER over 10 repetitions. The statistically best models are highlighted in bold.", "Rows_1": ["Syn1", "Syn2", "Syn3", "Syn4"], "Rows_2": ["RMSE", "REE"], "Column_1": ["LASSO", "L1 trace", "MMTFL", "GO MTL", "VSTG MTL k=1", "VSTG MTL k=3"]}, "indexed_table_339.csv": {"Caption": "Table 1: Results for the synthetic datasets showing the average RMSE and REE with 10 repetitions. The statistically best models are highlighted in bold.", "Rows_1": ["School exam", "Parkinson", "Computer survey", "MNIST", "USPS"], "Rows_2": ["RMSE", "ER"], "Column_1": ["LASSO", "L1+Trace", "MMTFL", "GO MTL", "VSTG MTL"]}, "indexed_table_50.csv": {"Caption": "Table 3: Model performance on the six petition datasets un- der ve metrics: the CMLRE outperformed others on four metrics out of ve.", "Rows_1": ["cMTFL I", "rMTFL", "JFS", "WELLSVM", "CMLRE"], "Column_1": ["ACC", "PR", "RE", "FS", "AUC"]}, "indexed_table_10.csv": {"Caption": "Table 4: Deep architecture analysis on BeerAdvocate.", "Rows_1": ["CMemNN C", "CMemNN"], "Column_1": ["Prec@1", "Prec@3", "Prec@5"]}, "indexed_table_105.csv": {"Caption": "Table 5: Deep architecture analysis on LibraryThing.", "Rows_1": ["CMemNN C", "CMemNN"], "Column_1": ["Prec@1", "Prec@3", "Prec@5"]}, "indexed_table_365.csv": {"Caption": "TSBRA information of processed dataset is listed in Table 2.", "Rows_1": ["HMM", "TimeSVD", "Aspect"], "Column_2": ["R", "P", "F1"], "Column_1": ["Top 10", "Top 5"]}, "indexed_table_335.csv": {"Caption": "Table4:Experiment Result on Last.fm", "Rows_1": ["TSBRA", "HMM", "TimeSVD", "Aspect"], "Column_2": ["R", "P", "F1"], "Column_1": ["Top 10", "Top 5"]}, "indexed_table_128.csv": {"Caption": "Table 4: Accuracy for pairs with overlaps of lengths \u2265 2000", "Rows_1": ["MHAP", "Minimap  default", "Minimap  alternative", "DALIGNER", "SmoothQGram"], "Column_2": ["Recall", "Precision", "F1 score"], "Column_1": ["E coli", "S cerevisiae", "Human"]}, "indexed_table_394.csv": {"Caption": "Table 2: Data stream characteristics and sizes", "Rows_1": ["Twitter", "IPv4 1#2", "IPv4 1#4", "IPv4 1#8", "IPv4 2#2", "IPv4 2#4", "IPv4 2#8", "IPv4 1#2", "IPv4 1#4", "IPv4 1#8", "IPv4 2#2", "IPv4 2#4", "IPv4 2#8"], "Column_2": ["items", "frequency", "size", "stream size"], "Column_1": ["Distinct", "Agg item", "Max item", "Flat stream", "Compressed"]}, "indexed_table_393.csv": {"Caption": "Table 6: Running times for all the algorithms, in seconds.", "Rows_1": ["Small scale data", "Airlines 700k", "Airlines 2M", "Airlines 5M"], "Column_1": ["MF", "oQPF", "Cpapprox", "Cpexact"]}, "indexed_table_321.csv": {"Caption": "Table A.1-1: P-values between the performance of DRVML vs. other methods on each dataset as calculated via Welch\u2019s two-sided t-test. Bold numbers denote higher performance by the proposed method. Values calculated on results shown in Table 3.", "Rows_1": ["Balance", "Credit", "Digits", "German", "Mice", "Segmentation", "Wisconsin"], "Column_1": ["KNN EUC", "LMNN", "RVML classes", "RVML transport"]}, "indexed_table_166.csv": {"Caption": "Table 2: AUC score of link prediction", "Rows_1": ["SC", "DeepWalk", "node2vec", "SDNE", "ANE", "NetRA"], "Column_1": ["UCI", "JDK"]}, "indexed_table_227.csv": {"Caption": "Table 2: Performance of clustering with outlier removal via di erent algorithms (%)", "Rows_1": ["ecoli", "yeast", "caltech", "sun09", "fbis", "k1b", "re0", "re1", "tr11", "tr23", "wap", "glass", "shuttle", "kddcup", "Average", "Score"], "Column_2": ["K-means", "K-means", "COR"], "Column_1": ["NMI", "Rn", "Jaccard", "F-measure"]}, "indexed_table_250.csv": {"Caption": "Table 4: Performance of outlier detection via di erent algorithms (%)", "Rows_1": ["ecoli", "yeast", "caltech", "sun09", "fbis", "k1b", "re0", "re1", "tr11", "tr23", "wap", "glass", "shuttle", "kddcup", "Average", "Score"], "Column_2": ["LOF", "COF", "LDOF", "FABOD", "iForest", "OPCA", "TONMF", "K-means", "COR"], "Column_1": ["Jaccard", "F-measure"]}, "indexed_table_383.csv": {"Caption": "Table 4: Ablation study on four datasets by NMI and ACC (red color denotes the best and blue the runner-up)", "Rows_1": ["GAE", "VGAE", "AGAE", "AGAE over GAE", "AGAE over VGAE"], "Column_2": ["AWA4K", "Dslr", "Pendigits", "TDT2", "avg"], "Column_1": ["Average Clustering Accuracy", "Normalized Mutual Information"]}, "indexed_table_388.csv": {"Caption": "Table 2: The performance of di erent community detection algorithms on real-world data sets with missing edges of 10% and 20%, respectively. Here the evaluation matrices, NMI, ARI and Purity are reported. We use bold font to mark the best results and underline the second best.", "Rows_1": ["Attractor", "Ncut", "Modularity", "Metis", "MCL", "Spectral", "PIC", "FUSE", "CLMC"], "Column_2": ["NMI", "ARI", "Pur"], "Column_1": ["Football", "Karate", "Polbooks"]}, "indexed_table_69.csv": {"Caption": "Table 3: DDI detection performance. Prec@k stands for precision@k where k equals to the number of known DDI pairs and AP is the average precision. Mean and standard de- viation of 5 random splits are given for the proposed method and baselines with similarity function. Baselines without similarity learning were reported for a single test split.", "Rows_1": ["Proposed", "DeepWalk Similarity", "node2vec Similarity", "LINE-1 Similarity", "LINE-2 Similarity", "LINE Similarity", "Deepwalk", "Node2vec", "LINE-1", "LINE-2", "LINE"], "Column_1": ["Prec@k", "AP"]}, "indexed_table_316.csv": {"Caption": "Table 2: Per edge type, micro-average, and macro-average MRR achieved by each model in the edge reconstruction task.", "Rows_1": ["Pretrained", "AspEm", "UniMetrics", "HEER"], "Column_2": ["Authorship", "Term usag", "Reference", "Pub venu", "Pub yea", "Micro average", "Macro average"], "Column_1": ["DBLP", "YAGO"]}, "indexed_table_421.csv": {"Caption": "Table 4: Comparisons of graph embeddings between MILE and its variants. Except for the original methods (DeepWalk and NetMF), the number of coarsening level m is set to 1 on PPI/Blog/Flickr and 6 on YouTube. Mi-F1 is the Micro-F1 score in 10_2 scale while Time column shows the running time of the method in minutes. \u201cN/A\u201d denotes the method consumes more than 128 GB RAM.", "Rows_1": ["DeepWalk", "MILE", "MILE rm", "MILE avg", "MILE untr", "MILE 2base", "MILE gs", "NetMF MILE", "MILE rm", "MILE proj", "MILE avg", "MILE untr"], "Column_2": ["Mi-F1", "Time"], "Column_1": ["PPI", "Blog", "Flickr", "YouTube"]}, "indexed_table_226.csv": {"Caption": "Table 4: Average RMSE on for CHI Crime Data. Le : RMSE on CDF, Right: RMSE on PDF. Unit: number of crimes.", "Rows_1": ["Group 1", "Group 2", "Group 3", "Average"], "Column_1": ["Single Node", "Joint Training", "GSRNN"]}, "indexed_table_168.csv": {"Caption": "Table 1: Comparison results (mean \u00b1 std.) of M3DN with both compared methods on 4 benchmark datasets. 6 commonly used criteria are evaluated. The best performance for each criterion is bolded. \u2191 /\u2193 indicate the larger/smaller the better of a criterion.", "Rows_1": ["M3 LDA", "MIMLmix", "CS3G", "DeepMIML", "M3MIML", "MIMLfast", "SLEEC", "Tram", "ECC", "ML KNN", "RankSVM", "ML SVM", "M3DN"], "Column_2": ["FLICKR25K", "IAPR TC 12 MS CoCo", "NUS WIDE", "IAPRTC 12 MS CoCo"], "Column_1": ["Coverage", "Macro AUC"]}, "indexed_table_233.csv": {"Caption": "Table 2: Comparison results (mean \u00b1 std.) of M3DN with com- pared methods on WKG Game-Hub dataset. 6 commonly used criteria are evaluated. The best performance for each criterion is bolded. \u2191 /\u2193 indicate the larger/smaller the better of a crite- rion.", "Rows_1": ["M3LDA", "MIMLmix", "CS3G", "DeepMIML M3MIML", "MIMLfast", "SLEEC", "Tram", "ECC", "ML KNN RankSVM", "ML SVM", "M3DN"], "Column_2": ["AUC", "AUC", "Loss", "Precision"], "Column_1": ["Coverage", "Macro", "Ranking", "Example", "Average", "Micro"]}, "indexed_table_413.csv": {"Caption": "Table 3: Comparison of accuracy values (%)", "Rows_1": ["lin", "wup", "SVM", "DT"], "Column_1": ["Agree", "Consc", "Extra", "Neuro", "Open", "Average"]}, "indexed_table_326.csv": {"Caption": "Table 3: Performance of all competing feature selection approaches", "Rows_1": ["Mediamill", "IMDB ECC F", "Corel16k010", "Corel16k011", "Corel16k012", "Corel16k013", "Corel16k014", "Corel16k015", "NUS WIDE", "EUR Lex", "bookmarks"], "Rows_2": ["IG PT", "RF PT", "MFNMI", "MDMR", "MIFS", "Our"], "Column_2": ["HamLoss", "F1", "Accuracy", "Precision", "Recall", "Hamloss"], "Column_1": ["Using basic classifier MLKNN", "Using basic classifier MLRDT"]}, "indexed_table_210.csv": {"Caption": "Table 2: Performance comparisons of BINN with baseline methods on two datasets (The improvements of RNN-based models over the best traditional method have been marked).", "Rows_1": ["P POP", "BPR MF", "Item KNN", "GRU4Rec", "GRU4Rec Concat", "HRNN Init", "HRNN All", "BINN"], "Column_2": ["Recall@20", "MRR@20"], "Column_1": ["Tianchi", "JD"]}, "indexed_table_148.csv": {"Caption": "Table 2: Experimental results of author node classi cation on the AMiner dataset.", "Rows_1": ["DeepWalk", "Node2vec", "LINE", "PTE", "Metapath2vec", "PMSW metapath", "PMSW metaschema"], "Column_1": ["Training 10%", "Training 30%", "Training 50%", "Training 70%", "Training 90%"]}, "indexed_table_330.csv": {"Caption": "Table 2: Recognition accuracy (%) and standard deviation of di erent methods on tra c video database, where Sparse noise, regular and irregular occlusion are added.", "Rows_1": ["Original", "Sparse", "Regular", "Irregular"], "Column_1": ["KNN", "ITML", "LMNN", "FANTOPE", "CAP", "RML", "Proposed"]}, "indexed_table_110.csv": {"Caption": "Table 3: Recognition accuracy (%) and standard deviation of di erent methods on OSR dataset, where Sparse noise, regular and irregular occlusion are added.", "Rows_1": ["Original", "Sparse", "Regular", "Irregular"], "Column_1": ["KNN", "ITML", "LMNN", "FANTOPE", "CAP", "RML", "Proposed"]}, "indexed_table_142.csv": {"Caption": "Table 4: Recognition accuracy (%) and standard deviation of di erent methods on Pub g dataset, where Sparse noise, regular and irregular occlusion are added.", "Rows_1": ["Original", "Sparse", "Regular", "Irregular"], "Column_1": ["KNN", "ITML", "LMNN", "FANTOPE", "CAP", "RML", "Proposed"]}, "indexed_table_307.csv": {"Caption": "Table3: Performance comparison with state-of-the-art Open IE systems on two datasets from di erent domains, using Precision@K, Mean Average Precision (MAP), Normalized Discounted Cumulative Gain (NDCG) and Mean Reciprocal Rank (MRR).", "Rows_1": ["ClausIE", "Stanford", "OLLIE", "MinIE", "ReMine R", "ReMine"], "Column_2": ["P@100", "P@200", "MAP", "NDCG@100", "NDCG@200", "MRR"], "Column_1": ["NYT", "Twitter"]}, "indexed_table_313.csv": {"Caption": "Table 3: Performance on the Three Real World Medical Datasets.", "Rows_1": ["LR Traditional", "RF Classification", "SVM", "GRU", "Deep RETAIN", "Learning LSTM", "CNN", "PRIMEr", "PRIMEc"], "Column_2": ["AUROC", "F1 Score", "Accuracy"], "Column_1": ["Heart Failure", "COPD", "Kidney Disease"]}, "indexed_table_206.csv": {"Caption": "Table 1: Evaluation of predictive performance and model complexity over 5-fold cross validation", "Rows_1": ["Ripper", "CBA", "SBRL", "BRS", "MARS"], "Column_2": ["acc", "ncond", "nfeat"], "Column_1": ["Juvenile", "Credit card", "Census", "Recidivism"]}, "indexed_table_46.csv": {"Caption": "Table 3: Overall performance of di erent models on Criteo, Dianping and Bing News datasets. The column Depth presents the best setting for network depth with a format of (cross layers, DNN layers).", "Rows_1": ["Model name", "LR", "FM", "DNN", "DCN", "Wide&Deep", "PNN", "DeepFM", "xDeepFM"], "Column_2": ["AUC", "Logloss", "Depth"], "Column_1": ["Criteo", "Dianping", "Bing News"]}, "indexed_table_386.csv": {"Caption": "Table 2: Prediction Performance on Benchmark Tasks. The two strongest performances are marked in bold.", "Rows_1": ["raw", "linear", "sigmoid", "relu", "linearm", "sigmoidm", "relum", "sigmoidmlp", "relumlp", "med2vec", "MiMEsum", "MiMEbp"], "Column_2": ["test loss", "ROC AUC", "test recall@5"], "Column_1": ["HF prediction", "Seq Dx prediction", "Med prediction"]}, "indexed_table_437.csv": {"Caption": "Table1: Spammer detection performance of the proposed framework. Each triple represents the performance: without adver- training (with l2 adversarial training, with l1 adversarial training).", "Rows_1": ["LR", "SVM", "RF", "NN", "ESM"], "Column_2": ["Prec", "Recall"], "Column_1": ["Twitter", "YelpReview"]}, "indexed_table_349.csv": {"Caption": "Table 2: Comparison of various algorithms using Top-N evaluation metrics. HOMF performs better than the methods compared consistently. Bold values indicate the best result among the methods considered. The exponential activation function was used to obtain the following results, and we set \u04341 (.) = \u04342 (.) for our graphs.", "Rows_1": ["MF", "Cofactor", "WMF", "MGCNN def", "MGCNN", "HOMF", "TrustWalker", "GRALS"], "Rows_2": [], "Column_2": ["@5", "@10"], "Column_1": ["Precision", "Recall", "MAP", "NDCG"]}, "indexed_table_55.csv": {"Caption": "Table 4: Comparison of top two performing algorithms using Top-N evaluation metrics at various percentage of training data", "Rows_1": ["HOMF", "WMF"], "Column_2": ["p@5", "p@10"], "Column_1": ["Precision", "Recall", "MAP", "NDCG"]}, "indexed_table_253.csv": {"Caption": "Table 2: Results for real-world graphs.", "Rows_1": ["Karate", "Lesmis", "Polbooks", "Adjnoun", "Football", "Jazz", "email Eu core", "Email", "Polblogs", "Wiki Vote", "ca HepTh", "ca HepPh", "ca CondMat", "AS 22july06", "email Enron", "web Stanford", "web NotreDame"], "Column_2": ["Ratio", "time", "Sampling"], "Column_1": ["Random", "Algorithm 1", "Algorithm 2"]}, "indexed_table_332.csv": {"Caption": "Table 4: NDCG@50, AUC and Mean Percentile Ranking (Section 5.1.3) of node recommendation. ** 0.01 level, paired t-test.", "Rows_1": ["LINE", "DeepWalk", "NetMF", "DCF", "AGH 1", "AGH 2", "IMH LE", "SH", "ITQ", "DNE MF i"], "Column_2": ["NDCG", "AUC", "MPR"], "Column_1": ["BlogCatalog", "PPI", "Wikipedia"]}, "indexed_table_434.csv": {"Caption": "Table 6: Evaluation results by our proposed model and com- pared methods on the VQA dataset.", "Rows_1": ["LSTM Q+I", "DPPnet", "FDA", "DMN+", "SMem", "SAN"], "Column_2": ["All", "Y N", "Num", "Other"], "Column_1": ["Open Ended", "Multi Choice"]}, "indexed_table_45.csv": {"Caption": "Table 1: Best test accuracies achieved by different methods on Fashion MNIST and relation extraction", "Rows_1": ["Mini 32", "Mini 64", "Mini 128", "Mini 256", "Manual", "BA"], "Column_2": ["Momentum", "SGD"], "Column_1": ["Fashion MNIST", "Relation extraction"]}, "indexed_table_130.csv": {"Caption": "Table 5: MAE summary for type of the day.", "Rows_1": ["Day", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday", "Average"], "Column_2": ["funPSF", "7 funPSF", "ANN", "ARIMA"], "Column_1": ["Regular days", "Festivities"]}, "indexed_table_129.csv": {"Caption": "Table 8: MAPE summary for type of the day.", "Rows_1": ["Day", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday", "Average"], "Column_2": ["funPSF", "7 funPSF", "ANN", "ARIMA"], "Column_1": ["Regular days", "Festivities"]}, "indexed_table_371.csv": {"Caption": "Table 2: Averages of ROUGE-2 Measures on Gay Rights (Column 2) and Abortion (Column 3); Median values of Relevance 886 829 _ate (Rel), Negative Predictive Value (NPV) and Clustering Accuracy Percentages 830 (Column 5) (values in \u201cbold\" represent best values disregarding Correct Summaries and PhAITVview values)  (Acc.) on GayRights (Column 4) and Abortion 887", "Rows_1": ["Random Summaries", "Correct Summaries", "JTV Extraction", "PhJTV Grouping Extraction", "AITV Extraction", "PhAITV Grouping Extraction", "PhAITV Extraction", "PhAITV Grouping LexRank", "TAM", "LAM LEX", "PhAITVview Grouping Extraction"], "Column_2": ["R2 R", "R2 P", "Rel", "NPV", "Acc"], "Column_1": ["Gay Rights", "Abortion"]}, "indexed_table_193.csv": {"Caption": "Table 1: Held_out document classi cation performance", "Rows_1": ["SLDA", "NA SLDA", "NA WE SLDA"], "Column_2": ["Accuracy", "Crosstnropy", "Crossntropy"], "Column_1": ["Amazon review", "Subjectivity"]}, "indexed_table_91.csv": {"Caption": "Table 3: Link prediction results on D1 and D2. Hits@N values are in %. For each measure, the best result is highlighted in bold.", "Rows_1": ["TransE", "HolE", "ComplEx", "Analogy", "DistMult E", "Amie Avg", "Amie Max", "Amie Prod", "Amie MLNs", "HornConcerto Avg", "HornConcerto Max", "HornConcerto Prod", "HornConcerto MLNs"], "Column_2": ["MRR", "Hits@1", "Hits@3", "Hits@10"], "Column_1": ["D1", "D2"]}, "indexed_table_114.csv": {"Caption": "Table 2: AUC scores for bipartite datasets, |Epos |/|E| = 0.3", "Rows_1": ["Movielens", "Condmat", "Frwiki"], "Column_1": ["Bi SFDP", "SFDP", "PA", "NMF", "SVD", "node2vec"]}, "indexed_table_366.csv": {"Caption": "Table 1: AUC and Detection Runtime of the Original Sp (ORG) and the REPEN-enabled Sp (REPEN). D is the dimensionality size and N is the data size. IMP and SU indicate the AUC improvement and speedup of REPEN over ORG, respectively.", "Rows_1": ["Data", "AD", "LC", "p53", "R8", "OT", "News20", "URL", "Webspam"], "Column_2": ["D", "N", "ORG", "REPEN", "IMP", "SU"], "Column_1": ["AUC", "Runtime"]}, "indexed_table_199.csv": {"Caption": "Table 3: e Harmonic Mean of Ranks (HMR) of network embedding methods and our itemset embedding methods on recom- mending an item for given itemset. A smaller HMR indicates a better recommendation method.", "Rows_1": ["LINE", "DeepWalk", "node2vec", "metapath2vec", "LearnSuc Pn", "LearnSuc Pt"], "Column_1": ["HMR", "HMR", "HMR", "HMR"]}, "indexed_table_170.csv": {"Caption": "Table 1: Representation Learning Experiment Results Comparison on Foursquare Network Dataset.", "Rows_1": ["seGEN", "seGEN", "seGEN HS", "seGEN BFS", "seGEN DFS", "seGEN NS", "seGEN LS", "LINE", "DeepWalk", "node2vec", "HPE", "seGEN", "seGEN HS", "seGEN BFS", "seGEN DFS", "seGEN NS", "seGEN LS"], "Column_2": ["5", "10", "1", "Detection", "25", "50"], "Column_1": ["AUC", "Prec@500", "Community", "Density", "Silhouette"]}, "indexed_table_149.csv": {"Caption": "Table 6: Accuracy (in terms of F1 and AUC) of the competing methods on four di erent datasets. DiffQue is run with its default con guration. Boldface numbers are the accuracy of the baseline methods using the con guration reported in the original papers. Blue (red) numbers are the accuracies of the best performing (second-ranked) method. We also measure the accuracy of each competing method using the network suggested by other methods and notice that most of the methods perform better if DiffQue\u2019s network is fed into their models (which indeed shows the superiority of our network construction mechanism).", "Rows_1": ["Network", "RCM", "SO1 Trueskill", "PageRank", "DiffQue", "SO2 Trueskill", "SO3 Trueskill", "MSE Trueskill"], "Column_2": ["HITS", "DiffQue", "RCM", "Trueskill", "PageRank"], "Column_1": ["F1", "AUC"]}, "indexed_table_229.csv": {"Caption": "Table 1: Performance comparison of the 7 models on predicting decompensation and length of stay.", "Rows_1": ["CNN RNN", "CNN RNN", "CNN AttRNN", "CNN MultiChAttRNN", "CNN LabMultiChAttRNN", "CNN IntAttMultiChRNN", "CNN IntLabMultiChAttRNN"], "Column_2": ["AUC ROC", "AUC PR", "Accuracy", "Kappa"], "Column_1": ["Decompensation", "Length of Stay"]}, "indexed_table_358.csv": {"Caption": "Table 3: Area Under the ROC Curve (AUC) at k = 100 and \u03b5 = 0.01 for prequentially evaluated classi ers (* p_value < 0.05)", "Rows_1": ["Adult 1", "Adult 2", "Electricity", "Airlines", "SEA", "Hyperplane", "Random RBF"], "Column_2": ["MiDiPSA", "KNN based", "FADS", "CASTLE", "Original", "Majority"], "Column_1": ["SGD", "Naive Bayes"]}, "indexed_table_198.csv": {"Caption": "Table 4: Performance comparison of AINCF without attention mechanism (NAINCF) and FISM", "Rows_1": ["FISM", "NAINCF", "MLP 0", "MLP 1", "MLP 2", "MLP 3"], "Column_2": ["HR", "NDCG"], "Column_1": ["Movielens 100k", "Epinions"]}, "indexed_table_214.csv": {"Caption": "Table 2: Performance comparison on Movielens100k dataset", "Rows_1": ["Item KNN", "BPR MF", "FISM", "AINCF"], "Column_2": ["HR", "NDCG"], "Column_1": ["Movielens 100k H=16", "Movielens 100k H=32", "Movielens 100k H=64"]}, "indexed_table_252.csv": {"Caption": "Table 6. Top -10 Higher Education Institutions, as ranked by the Open Ranking", "Rows_1": ["University of Cambridge", "Massachusetts Institute of Technology", "Stanford University", "Carnegie Mellon University", "University of Illinois At Urbana Champaign", "Georgia Institute of Technology", "Princeton University", "University of Hong Kong", "Purdue University", "University of Washington"], "Column_2": ["Productivity", "Quality", "Sustainability", "Factor", "Score"], "Column_1": ["Research", "Academic Academic", "International"]}, "indexed_table_293.csv": {"Caption": "Table 3: The results of AUC and Accuracy in CTR prediction.", "Rows_1": ["Ripple", "CKE", "SHINE", "DKN", "PER", "LibFM", "DeepWide"], "Column_2": ["AUC", "ACC"], "Column_1": ["Book Crossing", "Bing News"]}, "indexed_table_179.csv": {"Caption": "Table 2: Next-click prediction on 3 benchmark data sets.", "Rows_1": ["POP", "Item KNN", "FPMC", "GRU4Rec", "GRU4Rec+", "NARM", "STMO"], "Column_2": ["P@20", "MRR@20"], "Column_1": ["Yoochoose 1/64", "Yoochoose 1/4", "Diginetica"]}, "indexed_table_171.csv": {"Caption": "Table 5: Impacts of the last-click.", "Rows_1": ["STMP", "SRMP", "STAMP"], "Column_1": ["Yoochoose 1/64", "Yoochoose 1/4", "Diginetica"], "Column_2": ["P@20", "MRR@20"]}, "indexed_table_41.csv": {"Caption": "Table 5: The training and testing accuracy of all models.", "Rows_1": ["Accuracy", "LR", "LR F", "PLNN", "LR NS", "LR NSF", "PLNN NS"], "Column_2": ["Train", "Test"], "Column_1": ["FMNIST 1", "FMNIST 2"]}, "indexed_table_159.csv": {"Caption": "Table 3: Link prediction results on WN18, FB15K and WN18RR, FB15K-237 data sets.", "Rows_1": ["TransE", "DistMult", "ComplEx", "ConvE", "SGM SLC", "SGM SBC", "SGM SHC", "SGM ALC", "SGM ABC", "SGM AHC"], "Column_2": ["P@1", "P@3", "P@10"], "Column_1": ["FB15K", "FB15K 237", "WN18", "WN18RR"]}, "indexed_table_221.csv": {"Caption": "Table III. Performance Comparisons (A Smaller MAE or RMSE Value Means a Better Performance)", "Rows_1": ["Training 90%", "Training 80%"], "Rows_2": ["MAE", "RMSE"], "Column_1": ["Dimensionality =5"], "Column_2": ["User Mean", "Item Mean", "NMF", "PMF", "TCF", "Trust", "SoRec", "RSTE"]}, "indexed_table_186.csv": {"Caption": "Table III. Performance Comparisons (A Smaller MAE or RMSE Value Means a Better Performance)", "Rows_1": ["Training 90%", "Training 80%"], "Rows_2": ["MAE", "RMSE"], "Column_1": ["Dimensionality =10"], "Column_2": ["User Mean", "Item Mean", "NMF", "PMF", "TCF", "Trust", "SoRec", "RSTE"]}, "indexed_table_104.csv": {"Caption": "Table III. MAE Comparison with Other Approaches on Epinions Dataset", "Rows_1": ["5D", "10D"], "Rows_2": ["User Mean", "Item Mean", "Trust", "NMF", "SVD", "PMF", "SoRec"], "Column_1": ["90% Training", "80% Training", "70% Training", "60% Training"]}, "indexed_table_29.csv": {"Caption": "Table IV. RMSE Comparison with Other Approaches on Epinions Dataset", "Rows_1": ["5D", "10D"], "Rows_2": ["User Mean", "Item Mean", "Trust", "NMF", "SVD", "PMF", "SoRec"], "Column_1": ["90% Training", "80% Training", "70% Training", "60% Training"]}, "indexed_table_385.csv": {"Caption": "Table 2: Precision (%) and counts of promoted instances for each category using CPL, UPL, CSEAL, SEAL, and MBL.", "Rows_1": ["Predicate", "AcademicField", "Actor", "Animal", "Athlete", "AwardTrophyTournament", "BoardGame", "BodyPart", "Building", "Celebrity", "CEO", "City", "Clothing", "Coach", "Company", "Conference", "Country", "EconomicSector", "Emotion", "Food", "Furniture", "Hobby", "KitchenItem", "Mammal", "Movie", "NewspaperCompany", "Politician", "Product", "ProductType", "Profession", "ProfessionalOrganization", "Reptile", "Room", "Scientist", "Shape", "Sport", "SportsEquipment", "SportsLeague", "SportsTeam", "Stadium"], "Column_2": ["CPL", "UPL", "CSEAL", "SEAL", "MBL"], "Column_1": ["Precision", "Promoted Instances"]}, "indexed_table_101.csv": {"Caption": "Table 3: Precision (%) and counts of promoted instances for each relation using CPL, UPL, CSEAL, SEAL, and MBL.", "Rows_1": ["Predicate", "Company Acquired Company", "Athlete Plays For Team", "Athlete Plays In League", "Athlete Plays Sport", "CEO Of Company", "City Located In Country", "City Located In State", "Coach Coaches In League", "Coach Coaches Team", "Company Is In Economic Sector", "Company Competes With Company", "Company Has Office In City", "Company Has Office In Country", "Company Headquartered In City", "League Plays Games In Stadium", "Company Produces Product", "Product Instance Of Product Type", "Sport Uses Sports Equipment", "Stadium Located In City", "State Has Capital City", "State Located In Country", "Team Has Home Stadium", "Team Plays Against Team", "Team Has Home City", "Team Plays In League", "Team Plays Sport", "Team Won Award Trophy Tournament", "Average"], "Column_2": ["CPL", "UPL", "CSEAL", "SEAL", "MBL"], "Column_1": ["Precision", "Promoted Instances"]}, "indexed_table_230.csv": {"Caption": "Table 9: Comparison of retrieval results for a sample of web queries with either only unigram features (WSD-UNI), only bigram features (WSD-BI) or both.", "Rows_1": ["WSD", "WSD UNI", "WSD BI", "WSD ENDO", "WSD EXO"], "Column_2": ["DCG@1", "DCG@5", "DCG"], "Column_1": ["Len 2", "Len 3", "Len 4+"]}, "indexed_table_409.csv": {"Caption": "Table 5: Comparison of retrieval results for title (top table) and description (bottom table) TREC queries with query likelihood (QL), sequential dependence model (SD) and weighted sequential dependence model (WSD). Numbers in parentheses indicate % improvement in MAP over QL/SD (if available).", "Rows_1": ["QL 42-25", "SD 44-10", "SD 44-62", "QL 42-69", "SD 41-77"], "Column_2": ["prec@10", "b pref", "MAP"], "Column_1": ["ROBUST04", "W10g", "GOV2"]}, "indexed_table_15.csv": {"Caption": "Table 6: Comparison of retrieval results for title (left) and description (right) TREC queries with either only unigram features (WSD-UNI), only bigram features (WSD-BI) or both.", "Rows_1": ["WSD", "WSD UNI", "WSD BI", "WSD ENDO", "WSD EXO"], "Column_2": ["MAP"], "Column_1": ["ROBUST04", "W10g", "GOV2", "desc"]}, "indexed_table_112.csv": {"Caption": "Table 3: Evaluation measures for diversity-aware result rankings.", "Rows_1": ["MAP", "NDCG", "BPREF"], "Column_2": ["Original", "Voting", "Regression"], "Column_1": ["Google", "Bing Flickr"]}, "indexed_table_283.csv": {"Caption": "Table 6: Examples for MAP values of normal rankings for individual entities.", "Rows_1": ["Alfred Louis", "David Patterson", "Emmy Noether", "Ignatz Bubis", "Jon Huntsman", "Renate Blank", "Church of Christ Pantocrator", "San Lorenzo", "Pilatus", "M_nch", "William Vickrey", "Stephen Crabb", "Luisa Diogo", "Boyana Church", "Tre Cime di Lavaredo", "Aiguille dArgentire"], "Column_2": ["Original", "Voting", "Regression", "Regresssion"], "Column_1": ["Google", "Bing/Flickr"]}, "indexed_table_30.csv": {"Caption": "Table 3: Metrics comparison with URL rewrite approach for small data set", "Rows_1": ["Precision threshold", "1", ">= 95", ">= 9", ">= 80", "All"], "Column_2": ["Num Rule", "of Rules", "Reduction Ratio"], "Column_1": ["Rewrite approach", "Our approach"]}, "indexed_table_320.csv": {"Caption": "Table 5: Performance Comparisons (Dimensionality = 10)", "Rows_1": ["Douban", "Epinions"], "Rows_2": ["MAE", "Improve", "RMSE"], "Column_1": ["User Mean", "Item Mean", "NMF", "PMF", "RSTE", "SR1vss", "SR1pcc", "SR2vss", "SR2pcc"]}, "indexed_table_318.csv": {"Caption": "Table 3: Performance on relevance-based NDCG", "Rows_1": ["PMF", "EigenRank", "Assoc", "Freq"], "Column_2": ["NDCG@1", "NDCG@3", "NDCG@5"], "Column_1": ["Given 5", "Given 10", "Given 15"]}, "indexed_table_82.csv": {"Caption": "Table 3: The performance of different techniques averaged over a subset of queries with at least five random-walk candidates (1872 queries).", "Rows_1": ["ORG", "CombSUM", "CombRW", "RAPP L", "Merge", "RAPP"], "Column_2": ["NDCG@5", "NDCG@10"], "Column_1": ["1 Reformulation", "5 Reformulations"]}, "indexed_table_188.csv": {"Caption": "Table 5: Implicit Join Results", "Rows_1": ["Relation", "Players Teams", "Players Coaches", "Players Arenas", "Teams Arenas", "Teams Coaches", "Coaches Arenas", "Medicines Types", "Medicines Treatments", "Types Treatments"], "Column_2": ["Recall", "Prec", "F meas", "F Meas", "Avg", "Var"], "Column_1": ["Implicit Joins", "TextRunner", "Ling Seed"]}, "indexed_table_185.csv": {"Caption": "Table 3: Time until a video achieves at least 90% of its total views, across age (a) ranges (time nor- malized by video\u2019s lifetime, mean \u03bc, and standard deviation \u03c3).", "Rows_1": ["a < 7 days", "7 days < a < 1 month", "1 month < a < 1 year", "a > 1 year"], "Column_2": [], "Column_1": ["Top", "YouTomb", "Random"]}, "indexed_table_346.csv": {"Caption": "Table 1: Parameter estimates for distributions that best describe data.", "Rows_1": ["cascades", "cascade size", "spread", "diameter", "ave path length", "log of paths"], "Column_2": ["lk", "KS", "k", "xm_in"], "Column_1": ["Lognormal", "Weibull", "Power Law"]}, "indexed_table_377.csv": {"Caption": "Table2: Summary of User Attributes Across Clusters", "Rows_1": ["Attribute", "Number of Venues", "Percentage of Tips with Links", "Number of Dones and To Dos"], "Column_2": ["avg", "cv"], "Column_1": ["Cluster 0", "Cluster 1", "Cluster 2", "Cluster 3"]}, "indexed_table_96.csv": {"Caption": "Table 2: Statistics of the Senate and News testbeds", "Rows_1": ["Number of opinion words", "Number of documents", "Number of sentences", "Number of words", "Number of topic words"], "Column_2": ["Republican", "Democrat", "Total", "NYT", "Xinhua", "Hindu"], "Column_1": ["Senate", "News"]}, "indexed_table_158.csv": {"Caption": "Table 6: Comparison with the query expansion methods. Statistically significant difference of MSF over the baselines are marked using l, lw, and p, for LCE, LCE-WP and PQE baselines, respectively. Best result per column is marked by boldface. The numbers in parenthesis indicate improvement over the baseline with the best performance.", "Rows_1": ["LCE", "LCE WP", "MSF", "PQE"], "Column_2": ["nDCG@20", "MAP"], "Column_1": ["Robust 04", "Gov 2"]}, "indexed_table_376.csv": {"Caption": "Table 3: Prediction Generalization Power Comparison: T test < T train and T train = 9", "Rows_1": ["logistic", "GLM geo", "GLM exp", "GLM weib"], "Column_2": ["Accuracy", "AUC"], "Column_1": ["T test =1", "T test =2", "T test =3", "T test =4"]}, "indexed_table_259.csv": {"Caption": "Table 5: The effectiveness of different user profile-based metrics for predicting users\u2019 preference between clicked and skipped items in search results, as measured by proportion of clicked items with a correct preference prediction. Results are subdivided by degree of user profile focus based on profile distribution entropy. Prediction generally improved for users with more focused profiles, and when reading level and topic were used together.", "Rows_1": ["User Group", "Focused Profile", "Total"], "Column_2": ["Click", "DiffR", "KLR", "KLT", "KLR", "KLT", "KLRT"], "Column_1": ["Page based measures", "Site based measures"]}, "indexed_table_5.csv": {"Caption": "Table 8: Effectiveness of instance matching in terms R, P and F; * indicates statistically significant improvements of TYPiMatch over the best baseline, Supervised for AB, and PARIS for DS (paired t-test, p<0.05).", "Rows_1": ["AB", "DS"], "Column_2": ["R", "P", "F"], "Column_1": ["TYPiMatch", "Supervised", "PARIS"]}, "indexed_table_372.csv": {"Caption": "Table 1: Comparison of different community detection algorithms on the GeoLife data set between 2007-2009. The best performance is highlighted in bold.", "Rows_1": ["ObjectGrowth", "DSBM", "dIRM", "IC DRF"], "Column_2": ["Pearson", "INTRA", "INTER"], "Column_1": ["2007", "2008", "2009"]}, "indexed_table_196.csv": {"Caption": "Table 1: Precision and recall of predictions for several domains.", "Rows_1": ["Full model", "Frequency", "Co occurrence", "News alone", "News factual features", "News generalization"], "Column_2": ["Prec", "Recall"], "Column_1": ["General Predictions", "Death", "Disease Outbreak", "Riots"]}, "indexed_table_302.csv": {"Caption": "Table 3: Search ranking performance comparisons on AOL dataset and the commercial dataset in terms of MRR (larger is better) and NDCG@1, 3, 5. We report the performance under two different feature dimen- sionality (k = 200 and 300) for the four variations of the proposed method.", "Rows_1": ["BM25", "RW", "MCoC", "M PLS", "HGE 200", "HGE 300", "HGECG 200", "HGECG 300", "HGEEG 200", "HGEEG 300", "HGEFea 200", "HGEFea 300"], "Column_2": ["NDCG@1", "NDCG@3", "NDCG@5", "MRR"], "Column_1": ["AOL data", "Commercial data"]}, "indexed_table_113.csv": {"Caption": "Table 4: Object co-clustering performance comparisons on (a) AOL and (b) the commercial datasets in terms of Precision, Recall, F1 and Rand index (RI). We set the number of intents to be 20 and 50 (k = 20, 50).", "Rows_1": ["TFIDF", "PLSA", "BSGP", "MCoC", "HSoC", "HSoCFea"], "Column_1": ["performance on 20 clusters", "performance on 50 clusters"], "Column_2": ["Precision", "Recall", "F1", "RI"]}, "indexed_table_25.csv": {"Caption": "Table 4: Object co-clustering performance comparisons on (a) AOL and (b) the commercial datasets in terms of Precision, Recall, F1 and Rand index (RI). We set the number of intents to be 20 and 50 (k = 20, 50).", "Rows_1": ["TFIDF", "PLSA", "BSGP", "MCoC", "HSoC", "HSoCFea"], "Column_1": ["performance on 20 clusters", "performance on 50 clusters"], "Column_2": ["Precision", "Recall", "F1", "RI"]}, "indexed_table_161.csv": {"Caption": "Table 1: Image ranking performance (AP@100) on the Flickrgroup dataset. The one with best performance for each query is indicated by an oval box.", "Rows_1": ["balloon", "bird", "box", "car", "cat", "child", "flower", "guitar", "pumpkin", "waterfall", "average"], "Column_2": ["COT", "TF", "TF IDF"], "Column_1": ["SVMRank", "VisualRank", "SocialRank"]}, "indexed_table_218.csv": {"Caption": "Table 3: Network Quality Measures on Enron and Twitter", "Rows_1": ["Net LDA", "SN LDA", "TUCM", "TURCM 1", "TURCM 2", "Full TURCM", "CART", "CUT", "MMSB", "Naiive SN LDA"], "Column_2": ["N", "Cut Ja", "Jrm"], "Column_1": ["Enron Corpus", "Twitter Corpus"]}, "indexed_table_333.csv": {"Caption": "Table 4: The comparison of ranking performance in single domain based on Yahoo Set2. * indicates significantly better than baseline ListMLE at 99% confidence level according to paired two-tailed t-test", "Rows_1": ["ERR", "NDCG@1", "NDCG@3", "NDCG@5", "NDCG@10"], "Column_1": ["ListMLE", "CoList", "CoList"]}, "indexed_table_441.csv": {"Caption": "Table 2: Performance on Facebook, Google+, and Twitter. Higher is better. Standard errors are shown in parentheses. The best and second best methods are annotated as \u20181\u2019 and \u20182\u2019.", "Rows_1": ["MMSB", "Clique percolation", "Link clustering", "BigCLAM", "DEMON", "NMF", "CoDA undirected", "CoDA directed"], "Column_2": ["Google+", "Twitter", "Facebook", "Average"], "Column_1": ["F1 score", "Jaccard similarity"]}, "indexed_table_327.csv": {"Caption": "Table 3: Performance Comparison", "Rows_1": ["Popularity", "Co Click", "NMF", "Hybrid SVM", "HeteRec g", "HeteRec p"], "Column_2": ["Prec1", "Prec5", "Prec10", "MRR"], "Column_1": ["IM100K", "Yelp"]}, "indexed_table_26.csv": {"Caption": "Table 6: The per-category AUC for all the proposed approaches and the baseline.", "Rows_1": ["Des&Mult", "Soft Dev", "Web Dev", "Writing"], "Column_1": ["Feedback", "Ranker", "Logit", "Bayes Net"]}, "indexed_table_78.csv": {"Caption": "Table 6: Comparison of gloss finding methods with 10% unambiguous abstracts used as training data. GLOFIN-NB always gives best or near-best F1 scores. Please refer Section 6.1.1 for more details.", "Rows_1": ["SVM", "Label Propagation", "GLOFIN NB"], "Column_1": ["NELL DATA", "Freebase Dataset"], "Column_2": ["Precision", "Recall", "F1"]}, "indexed_table_6.csv": {"Caption": "Table 1: Average errors of estimating past locations and travel times.", "Rows_1": ["Full Model", "common", "GPS speed", "Shortest path"], "Column_2": ["SF", "NYC", "Boston", "Salina"], "Column_1": ["time error", "location error", "heading direction error", "heading direction error"]}, "indexed_table_51.csv": {"Caption": "Table 3: Performance Comparison of Negative Link Prediction in Epinions and Slashdot.", "Rows_1": ["random", "sPath", "negIn", "negInS", "NeLP negIn", "NeLP"], "Column_2": ["F1", "Precision"], "Column_1": ["Epinions", "Slashdot"]}, "indexed_table_68.csv": {"Caption": "Table5: RMSE comparison of the LPF model and several baseline approaches in the dynamic setting, where the app opens in month t + 1 are predicted using the app open observations in month t. App opens were aggregated over 14,856 devices and 100 locations.", "Rows_1": ["TS 1", "TS 2", "TS 3", "TS 4", "TS 5", "TS 6"], "Column_1": ["LPF", "BPF", "PM", "NMF", "Zero", "Global", "Loc Mean", "Item Mean", "MFSI"]}, "indexed_table_107.csv": {"Caption": "Table 3: Temporal ranking methods results. Symbols _ and * stand for a p < 0.05 statistical significant improvement over KDE(score) and LTR respectively (_ and * for p < 0.01).", "Rows_1": ["BM25", "IDF", "LM Dir", "Recency", "KDE", "LTR"], "Column_1": ["Temporal", "Atemporal", "T+A", "All"], "Column_2": ["MAP", "P30"]}, "indexed_table_258.csv": {"Caption": "Table 5: Performance of Assignment Grade Prediction with Different Methods (%).", "Rows_1": ["Science", "Non Science"], "Rows_2": ["LRC", "SVM", "FM", "LadFG"], "Column_1": ["AUC", "Precision", "Recall", "F1 score"]}, "indexed_table_451.csv": {"Caption": "Table 3: Comparison with different baselines.", "Rows_1": ["Random", "Retweet Ratio", "Volume", "Bag of Words", "Combine", "Our Method"], "Column_2": ["AUC", "F1"], "Column_1": ["Elections", "Gun Rights"]}, "indexed_table_317.csv": {"Caption": "Table 7: Effects of using tied weights on MovieLens data. \u201cTW\u201d means using tied weights, while \u201cNTW\u201d means no tied weights.", "Rows_1": ["M1", "M2", "M3", "M4"], "Column_2": ["TW", "NTW"], "Column_1": ["MAP@1", "MAP@5", "MAP@10"]}, "indexed_table_234.csv": {"Caption": "Table 2: Performance of CCA on different testing data (see following sections for detailed description of the data).", "Rows_1": ["Getty", "Twitter", "AMT Twitter"], "Column_1": ["Precision", "Recall", "F1", "Accuracy"]}, "indexed_table_359.csv": {"Caption": "Table 3: Performance on the testing data set by different approaches.", "Rows_1": ["Textual", "Visual", "Early Fusion", "Late Fusion", "CCR"], "Column_1": ["Precision", "Recall", "F1", "Accuracy"]}, "indexed_table_244.csv": {"Caption": "Table 4: Performance on the Twitter testing data set by different approaches.", "Rows_1": ["Textual", "Visual", "Early Fusion", "Late Fusion", "CCR"], "Column_1": ["Precision", "Recall", "F1"]}, "indexed_table_182.csv": {"Caption": "Table 5: Performance on the AMT manually labeled data set by different approaches.", "Rows_1": ["Textual", "Visual", "Early Fusion", "Late Fusion", "CCR"], "Column_1": ["Precision", "Recall", "F1", "Accuracy"]}, "indexed_table_411.csv": {"Caption": "None", "Rows_1": ["Irvine", "ICWSM", "MemeTracker", "Digg", "Twitter", "LastFM"], "Rows_2": ["IC", "NetRate", "CTIC", "Embedded IC"], "Column_1": ["MSE", "LogLikelihood", "MAP", "F1", "nbParams"]}, "indexed_table_369.csv": {"Caption": "Table 3: Evaluation results on Yahoo data and Baidu data (10 million)", "Rows_1": ["LMIR", "LMCleaf", "Skipcat trlm", "DRLM"], "Column_1": ["Yahoo data", "Baidu data"], "Column_2": ["MAP", "MRR", "R Prec", "P@1"]}, "indexed_table_141.csv": {"Caption": "Table 5: Running time (seconds) for various methods.", "Rows_1": ["QUIS", "PMIA", "DDIC", "PR", "DEG"], "Column_1": ["DBLP", "Twitter"], "Column_2": ["Query", "Total"]}, "indexed_table_238.csv": {"Caption": "Table 1: AUC computed for PR plots for six different computer science fields in 2005", "Rows_1": ["QUIS", "PMIA", "DDIC", "PR", "DEG", "WTDDEG"], "Column_1": ["DM", "NW", "ML", "CV", "HW", "SE"]}, "indexed_table_428.csv": {"Caption": "Table 2: AUC computed for PR plots for various computer sci- ence fields in 2011", "Rows_1": ["QUIS", "PMIA", "DDIC", "PR", "DEG", "WTDDEG"], "Column_1": ["DM", "NW", "ML", "CV", "HW", "SE"]}, "indexed_table_447.csv": {"Caption": "Table 3: CCCF vs. No Co-Clustering on the Lastfm data", "Rows_1": ["P@10", "R@10", "F1@10", "MAP@10"], "Column_2": ["NONE", "CCCF"], "Column_1": ["POP", "ITEMCF", "MF", "WARP"]}, "indexed_table_63.csv": {"Caption": "Table 1: Test log-likelihood on rank aggregation datasets.", "Rows_1": ["PEER POSTER", "PEER FINAL", "MOVIELENS", "JESTER", "SUSHI_A", "SUSHI_B", "ELECTION_A5", "ELECTION_A9", "ELECTION_A17", "ELECTION_A48", "ELECTION_A81", "ELECTION_SF", "ELECTION_CM", "ELECTION_DW", "ELECTION_DN"], "Column_1": ["NAIVE", "B T", "OUR BEST"]}, "indexed_table_308.csv": {"Caption": "Table 2: Comparison of recommendation algorithms in terms of NDCG@3, NDCG@5 and NDCG@7 on Del.icio.us dataset. The results of 80%, 90% and 100% training data are reported.", "Rows_1": ["MIOE", "TVS", "CVS", "User CF", "Funk SVD"], "Column_2": ["NDCG@3", "NDCG@5", "NDCG@7"], "Column_1": ["80% Training Data", "90% Training Data", "100% Training Data"]}, "indexed_table_222.csv": {"Caption": "Table 4: Retrieval effectiveness of retrieval methods CTRank and StructRank, for queries of different length. The _ denotes statistically significant difference with CTRank method (Wilcoxon sign test, \u03b1 < 0.05). The numbers in the parentheses indicate % improvement of StructRank over CTRank baseline.", "Rows_1": ["CTRank", "StructRank"], "Column_2": ["nDCG@1", "nDCG@5", "nDCG@10"], "Column_1": ["Len 1", "Len 2+3"]}, "indexed_table_264.csv": {"Caption": "Table 1: MM is on average rated higher, deemed under- or over-clustered for fewer queries (only 25% combined), and found to be the best for many more queries than DC and SQ (over 400 queries each with 3 evaluators).", "Rows_1": ["MM", "DC", "SQ"], "Column_2": ["Mean", "Bounds", "Ratings Queries"], "Column_1": ["Overall Ratings", "Under Clustered", "Over Clustered", "Best", "Worst"]}, "indexed_table_8.csv": {"Caption": "Table 2: Location estimation accuracy of earthquakes from tweets. For each method, we present the difference of the estimated latitude and the longitude to the actual ones, and their Euclidean distance. Smaller distance means better performance.", "Rows_1": ["Aug 10 01:00", "Aug 11 05:00", "Aug 13 07:50", "Aug 17 20:40", "Aug 18 22:17", "Aug 21 08 51", "Aug 24 13:30", "Aug 24 14:40", "Aug 25 02:22", "Aug 25 20:19", "Aug 31 00:46", "Aug 31 21:11", "Sep 3 22:26", "Sep 4 11:30", "Sep 05 10:59", "Sep 08 01:24", "Sep 10 18:29", "Sep 16 21:38", "Sep 22 20:40", "Oct 1 19:43", "Oct 5 09:35", "Oct 6 07:49", "Oct 10 17:43", "Oct 12 16:10", "Oct 12 18:42", "Oct 7 12:00", "Oct 7 15:00", "Oct 7 18:00", "Oct 7 21:00", "Oct 8 0:00", "Oct 8 6:00", "Oct 8 9:00", "Oct 8 12:00", "Oct 8 15:00", "Oct 8 18:00", "Oct 8 21:00"], "Column_2": ["lat", "long", "dist"], "Column_1": ["Actual center", "Median", "Weighted ave", "Kalman filters", "Particle filters"]}, "indexed_table_431.csv": {"Caption": "Table 4: Precision/Recall of Inferred Query Relationships", "Rows_1": ["Accuracy", "unrelated", "belongs", "equivalent"], "Column_2": ["prec", "rec", "F1", "prec"], "Column_1": ["By munually labeled data", "By mechanical turk data"]}, "indexed_table_116.csv": {"Caption": "Table 2: Computation Breakdowns with Different __ and __", "Rows_1": ["Component", "X=WTA", "Y=WTWH", "H=H/Y"], "Column_2": ["Shuffle", "Telapse"], "Column_1": ["k=8", "k=32", "k=128"]}, "indexed_table_334.csv": {"Caption": "Table 1: Prediction Impact By Corpus.", "Rows_1": ["Total", "Positive", "Negative", "Neutral"], "Column_2": ["Count"], "Column_1": ["TV", "YouTube", "Google"]}, "indexed_table_76.csv": {"Caption": "Table 3: Statistical properties of selected folksonomy parti- tions. %t denotes the fraction of the tags from the complete dataset included in the respective partition; %w denotes the number of similar tag pairs (__,__sim) found in WordNet for the respective partition divided by the number of mapped pairs from the whole dataset. For the entire dataset, ____ = 9944 and wn_pairs(__ ) = 4335.", "Rows_1": ["i", "1", "3", "5", "10", "20", "50", "70"], "Column_2": ["%t", "%w"], "Column_1": ["DF-i", "DFi"]}, "indexed_table_79.csv": {"Caption": "Table 3: Kendall\u2019s \u03c4 correlation and the relative errors for prediction w.r.t. the cascade measures comparing with the ones for prediction w.r.t. AP.", "Rows_1": ["Kendall", "RMSR", "MARE"], "Column_2": ["AP I", "NRBP", "\u4f2a DCG", "ERR I"], "Column_1": ["w r t ERR IA", "w r t NRBP"]}, "indexed_table_58.csv": {"Caption": "Table 8. Effect of Pruning", "Rows_1": ["w/o pruning", "w/ pruning"], "Column_2": ["R@1", "R@10", "P@1", "P@10", "MKS", "PMKS"], "Column_1": ["all queries", "misspelled queries"]}, "indexed_table_289.csv": {"Caption": "Table 1: US Mobile Query Categorization", "Rows_1": ["Arts & Humanities", "Automotive", "Consumer Goods", "Entertainment", "Finance", "Government&Politics", "Health & Pharma", "Hobbies", "International Interest", "Life Stages", "Local", "Miscellaneous", "News", "People", "Reference", "Religion", "Retail", "Science Small Business", "Sports", "Electronic Gadgets", "Telecommunications"], "Column_2": ["%of unique queries", "%of all queries"], "Column_1": ["2010 samples", "2007 samples"]}, "indexed_table_443.csv": {"Caption": "Table 1: Statistics of the Yahoo Competition and Microsoft Learning to Rank data sets.", "Rows_1": ["# Features", "# Documents", "# Queries", "Avg # Doc per Query"], "Column_2": ["Set 1", "Set", "F1", "F2", "F3", "F4", "F5"], "Column_1": ["Yahoo LTRC", "MSLR MQ2008 Folds"]}, "indexed_table_39.csv": {"Caption": "Table 2: Results in ERR and NDCG on the Yahoo and Microsoft data sets. The number of boosting iterations is selected with the validation data set. On both Yahoo sets, pGBRT matches the result of GBRT with d = 4 when the tree depth is increased. For the Microsoft sets, the ranking results tend to be slightly lower.", "Rows_1": ["GBRT", "pGBRT", "pGBRT", "pGBRT"], "Column_2": ["Set", "F1", "F2", "F3", "F4", "F5"], "Column_1": ["Yahoo LTRC", "MSLR MQ2008 Folds"]}, "indexed_table_276.csv": {"Caption": "Table 1: Information retrieval and dissemination with a creation rate of 25 blogs per second for various network sizes - \u03bca is the average retrieval accuracy, \u03bcr is the average replication rate (% of network), and \u03c3 denotes the corresponding standard deviation.", "Rows_1": ["1", "2", "3"], "Column_2": ["a", "r"], "Column_1": ["|T |", "Retrieval", "Dissemination"]}, "indexed_table_44.csv": {"Caption": "Table 1: Data summary: G and Y indicate total # of Google and Yahoo news results evaluated by Amazon Turkers; T indicates # of tweets per day", "Rows_1": ["Query", "google", "egypt", "siri", "tax", "greece/greek", "election", "nba", "education", "financial/finance k documents containing qi returned from news search engine kobe bryant", "military/army", "revolution", "economy/economic", "vacation", "insurance", "obama", "ncaa", "cnn", "christmas", "iran", "R is provided by Turkers from three states discount", "britney spears", "clinton", "debt", "republication", "ing CTVM ranking with local state tweets and CTVM rank euro", "lady gaga", "lebron james", "stock", "nfl", "health care", "china", "gay/lesbian"], "Column_2": ["G", "Y", "T"], "Column_1": ["CA", "NY"]}, "indexed_table_124.csv": {"Caption": "Table 2: URL number of click prediction Error (lower numbers indicate higher performance). Statistically significant results are shown in bold.", "Rows_1": ["Query Type", "General", "Dynamic", "Temp Reform", "Alternating"], "Column_2": ["AVG", "LIN", "POW", "YES", "SMOOTH", "TREND", "PERIODIC", "TREND+"], "Column_1": ["Baselines", "Temporal SSM Models"]}, "indexed_table_398.csv": {"Caption": "Table 1: Query number of click prediction Error (lower numbers indicate higher performance). Statistically significant results are shown in bold.", "Rows_1": ["Query Type", "General", "Dynamic", "Temp Reform", "Alternating"], "Column_2": ["AVG", "LIN", "POW", "YES", "SMOOTH", "TREND", "PERIODIC", "TREND+", "SURPRISE", "DML", "BIC"], "Column_1": ["Baselines", "Temporal SSM Models", "Model Selection"]}, "indexed_table_59.csv": {"Caption": "Table 1: Web sites used in measuring energy consumption", "Rows_1": ["m wsj com news page", "m gmail com inbox", "m picasa com user albums", "m aol com portal home", "m amazon com product page", "mobile nytimes com US home page", "microsoft touch facebook com facebook wall", "mw weather com Stanford weather", "apple com home page", "m imdb com movie page", "m microsoft com home page", "m natgeo com home page", "m wikipedia org article page", "bbc com mobile home page", "m ebay com product page", "m yahoo com portal home", "m youtube com home page", "baidu com search page", "blogger com home page", "m cnn com headlines page", "m engadget com portal page", "m go com start page", "m live com personal page", "wordpress com home page", "tumblr com home page"], "Column_2": ["life", "Upload", "Download"], "Column_1": ["Battery", "Traffic"]}, "indexed_table_270.csv": {"Caption": "Table 2: Performance Comparisons on MovieLens-100K in terms of Precision, F1 and NDCG. 15 and 25 subgroups are used for our approach. The bold number indicates an obvious improvement (difference \u2265 0.01).", "Rows_1": ["POP", "User based", "Item based", "SVD", "NMF", "MMMF", "NPCA", "PD"], "Column_2": ["P@10", "NDCG@10", "F1@10"], "Column_1": ["base performance", "performance on 15 subgroups", "performance on 25 subgroups"]}, "indexed_table_212.csv": {"Caption": "Table 3: Performance Comparisons on MovieLens-1M in terms of MAP and NDCG. 5, 15 and 25 subgroups are used for our approach. The bold number indicates an obvious improvement (difference \u2265 0.01).", "Rows_1": ["POP", "User based", "Item based", "SVD", "NMF", "MMMF", "NPCA", "PD"], "Column_2": ["NDCG@10", "MAP"], "Column_1": ["base", "5 subgroups", "15 subgroups", "25 subgroups"]}, "indexed_table_200.csv": {"Caption": "Table 4: Performance Comparisons on Lastfm in terms of MAP and NDCG. 30 subgroups are used for our approach. The left part is the result of using original data and the left part is the result of using normalized data. The bold number indicates an obvious improvement (difference \u2265 0.01) and symbol * indicates a big improvement (difference \u2265 0.05).", "Rows_1": ["POP", "User based", "Item based", "SVD", "NMF", "MMMF", "NPCA"], "Column_2": ["NDCG@10", "MAP"], "Column_1": ["Original data base", "Original data base 30 subgroups", "Rescaled data base", "Rescaled data 30 subgroups"]}, "indexed_table_138.csv": {"Caption": "Table 1: MAP and NDCG Results For Invitation Graph, Navigation Graph, and Hybrid PageRank(PR) and Log Fair Bets (LFB) approaches. The values in parentheses give the percentage improvement over Invitation Graph PageRank, treated as a baseline approach.", "Rows_1": ["Metric", "MAP@1000", "MAP@100K", "MAP@1mil", "NDCG@1000", "NDCG@100K", "NDCG@1mil"], "Column_2": ["PR", "LFB", "Borda P", "Bimodal PR", "Borda LF", "Bimodal LF"], "Column_1": ["Invitation Graph", "Navigation Graph", "Hybrid"]}, "indexed_table_282.csv": {"Caption": "Table 4: User study results of the user study. The mean and standard deviation (SD) are shown as \u201c<Mean> (<SD>)\u201d. A dagger in the Significance column indicates significant interaction between the two interfaces in Entity Comparison tasks.", "Rows_1": ["# Answers", "# Queries", "# QS queries", "# Documents", "# Entities", "Successful search rate", "Familiarity", "Easiness", "Satisfaction", "Enough time", "Helpfulness"], "Column_2": ["List", "SParQS"], "Column_1": ["Information Gathering", "Entity Comparison"]}, "indexed_table_323.csv": {"Caption": "Table 3: Summary of the performances on DS1 development and test data set. 6F stands for the features mentioned in 3.2", "Rows_1": ["6F+2500*AR", "AR", "6F+1000*AR", "6F+AR", "6F", "6F+100*AR", "6F+10*AR", "Systems", "Best of Ours", "Only AR feature", "Baseline system"], "Column_2": ["MRR", "P@1", "P@2", "Feature se"], "Column_1": ["SVMRank", "ListNet"]}, "indexed_table_342.csv": {"Caption": "Table 1: Upper bound of the browser delay reduction from speculative loading under different cache states (in ms)", "Rows_1": ["Sites", "ESPN", "CNN", "Google", "Yahoo Mail", "Weather", "Craigslist", "Neopets Games", "Varsity Tutors", "Ride METRO", "Rice Registrar", "Average"], "Column_2": ["Legacy", "Speculate", "Reduction"], "Column_1": ["Fresh Cache", "Expired Cache", "Empty Cache"]}, "indexed_table_86.csv": {"Caption": "Table 2: Browser delay reduction from speculative loading for webpage revisits under different cache states (in ms)", "Rows_1": ["Sites", "ESPN", "CNN", "Google", "Yahoo Mail", "Weather", "Craigslist", "Neopets Games", "Varsity Tutors", "Ride METRO", "Rice Registrar", "Average", "Neopets"], "Column_2": ["Legacy", "Tempo Reduction"], "Column_1": ["Fresh Cache", "Expired Cache", "Empty Cache"]}, "indexed_table_442.csv": {"Caption": "Table 1: Compression results of HDT for several datasets.", "Rows_1": ["wikipedia", "dbtune", "uniprot", "dbpedia en"], "Column_2": ["gzip", "bzip2", "HDT"], "Column_1": ["Triples", "Size", "Compression"]}, "indexed_table_262.csv": {"Caption": "Table 3: Performance in Random and Stylized Networks", "Rows_1": ["Erdos Renyi", "Small World", "Scale Free"], "Column_2": ["Avg Nodes", "Avg Degree", "Diameter", "Path length"], "Column_1": ["Avg Nodes", "Avg Degree", "Diameter", "Path length"]}, "indexed_table_243.csv": {"Caption": "Table 1: Comparison of Top-N Recommendation Algorithms", "Rows_1": ["SLIM", "SSLIM1", "SSLIM2", "itemSI", "CWRMF"], "Column_2": ["HR", "ARHR"], "Column_1": ["ML100K", "NF", "CrossRef", "Lib", "Pubmed", "DrugSE"]}, "indexed_table_408.csv": {"Caption": "Table 1: Top 10 cities (number of tweets).", "Rows_1": ["city", "New York", "London", "S Paulo", "Los Angeles", "Amsterdam", "Guarulhos", "Osasco", "S Bernardo", "Rotterdam", "Mexico City"], "Column_2": ["tweets", "city"], "Column_1": ["# now playing", "# itunes"]}, "indexed_table_433.csv": {"Caption": "Table 3: Percentage of correct and incorrect loca- tions found by three tools by granularity of locations and by hashtags. TP represents True Positive and FN is False Negative.", "Rows_1": ["Country", "State", "City", "Area", "Suburb", "POI", "Hashtag"], "Column_2": ["NER", "NLP", "PM"], "Column_1": ["Stanford", "Twitter", "Yahoo"]}, "indexed_table_279.csv": {"Caption": "Table 5: Best performance achieved in LMF with corresponding density requirement \u03c1_ and number of diagonal blocks k. Bold numbers indicate improvements that are \u2265 0.01 on MovieLens and DianPing or \u2265 0.2 on Yahoo!Music. The standard deviations are \u2264 0.002 on MovieLens and DianPing and \u2264 0.01 on Yahoo!Music.", "Rows_1": ["SVD", "NMF", "PMF", "MMMF"], "Column_2": ["k", "RMSE"], "Column_1": ["MovieLens 100K", "MovieLens 1M", "DianPing", "Yahoo Music"]}, "indexed_table_32.csv": {"Caption": "Table 3: Comparison of different algorithms on book author and Flickr data set. On book author data set, the algorithms are compared by their accuracies. On Flickr data set, the algorithms are compared by their average precisions and recalls on 12 tags.", "Rows_1": ["Voting", "Estimates", "TruthFinder", "Accu", "MSS"], "Column_2": ["accuracy", "precision", "recall"], "Column_1": ["book author data set", "Flickr data set"]}, "indexed_table_399.csv": {"Caption": "Table 1: Experimental evaluation of score functions", "Rows_1": ["dist", "cdist", "tdist", "ad", "tfidf", "collab", "SoCScore"], "Column_2": ["TopScore@10", "Recall@10", "MRR"], "Column_1": ["all collaborators", "only new collaborators"]}, "indexed_table_374.csv": {"Caption": "Table 2: Performance comparison on the Douban dataset", "Rows_1": ["SoCo", "SoReg", "RPMF", "BMF", "Item based CF", "User based CF"], "Column_2": ["MAE", "RMSE"], "Column_1": ["Book", "Movie", "Music", "All"]}, "indexed_table_74.csv": {"Caption": "Table 7: Average accuracy of SVM classifier trained on differ- ent feature sets for overall rating prediction", "Rows_1": ["BOW SVM", "LDA SVM", "D PLDA SVM", "FLDA SVM"], "Column_2": ["cold", "non"], "Column_1": ["Epinions", "Amazon", "TripAdvisor"]}, "indexed_table_89.csv": {"Caption": "None", "Rows_1": ["All", "Music", "Entertainm", "Comedy", "People", "Film", "Education", "Animals", "News", "Shows"], "Column_2": ["n", "abs", "rmse"], "Column_1": ["baseline", "SVM"]}, "indexed_table_392.csv": {"Caption": "Table 1: Results obtained for the tested ranking models.", "Rows_1": ["nDCG@1", "nDCG@5", "nDCG@10", "P@1", "P@5", "P@10", "S@1", "S@5"], "Column_2": ["Lucene", "NutchWAX", "MdTREC", "TVersions", "TSpan", "MdRankBoost"], "Column_1": ["time unaware", "time aware"]}, "indexed_table_280.csv": {"Caption": "Table 2: Performance comparisons of top-n recommendation on Epinions in terms of MAP, F1 and nDCG.", "Rows_1": ["RANDOM", "PMF", "PMF D", "MCoC", "TopRec S", "TopRec M", "TopRec Net"], "Column_2": ["MAP", "F1", "nDCG"], "Column_1": ["n=5", "n=10", "n=15", "n=20"]}, "indexed_table_402.csv": {"Caption": "Table 1: Sentence selection evaluation results", "Rows_1": ["Random", "LexRank", "MEAD", "Chieu", "Ours"], "Column_2": ["R1", "R2", "SU4"], "Column_1": ["Timeline 28", "Timeline 30"]}, "indexed_table_418.csv": {"Caption": "Table 1: Structural properties of dream interpreta- tion networks generated by backbone extraction for appropriate values of the parameter \u03b1.", "Rows_1": ["# nodes", "# edges", "Density", "Avg degree", "Avg strength", "Clustering coefficient", "Avg shortest path"], "Column_2": ["w=05", "w=15"], "Column_1": ["English", "Chinese", "Arabic"]}, "indexed_table_180.csv": {"Caption": "Table 11 The quality of picture tag types descriptive analysis", "Rows_1": ["vocabularies used in the image title", "vocabularies to describe the image contents", "vocabularies to describe the image type", "release location of the image", "vocabularies to self organizing", "self feelings after watching the image", "image promulgator", "source of the image", "release time of the image", "equipment used to take the image"], "Column_2": ["M value", "SD value"], "Column_1": ["Tag Quality"]}, "indexed_table_309.csv": {"Caption": "Table 1: Test set accuracy for various prediction tasks. Latent Collaborative Clustering (LCC) performs the best in all cases. Note that conventional feature-based approaches are ill-suited for the HOLD ONE CLUSTER task (see Section 4.1.3).", "Rows_1": ["Random", "Largest Cluster", "Feature 1", "Feature 2", "Transformed Feature 1", "Transformed Feature 2", "LCC", "Augmented LCC"], "Column_1": ["HOLD 50%", "HOLD 25% PER CLUSTER", "HOLD ONE CLUSTER"]}, "indexed_table_33.csv": {"Caption": "Table 1: Performance of various EL systems on the MSNBC, AQUAINT, and ACE2004 datasets.", "Rows_1": ["PriorProb", "Local", "Cucerzan", "M&W", "Han_11", "GLOW", "RI", "SemSig"], "Column_2": ["Accuracy", "MI-F1", "MA-F1"], "Column_1": ["MSNBC", "AQUAINT", "ACE2004"]}, "indexed_table_100.csv": {"Caption": "Table 3: Advertising abuses discovered for each examined service, separated by the geographical location of our monitors", "Rows_1": ["Service", "adf ly", "linkbucks com", "adfoc us", "bc vc", "ysear ch", "coinurl lcom", "reducelnk com", "ssl gs", "zpag es", "adcrun ch"], "Column_2": ["#Ad Clusters", "#Malicious clusters", "% Impressions", "#Adult clusters", "%Impressions"], "Column_1": ["EU Monitor", "US Monitor"]}, "indexed_table_191.csv": {"Caption": "Table 2: Test performance in terms of average precision and NDCG@10. Higher values mean better perfor- mance. Bold faces mean that the method performs statistically significantly better in the setting, at the level of 95% confidence level.", "Rows_1": ["MovieLens", "EachMovie", "Yelp"], "Rows_2": ["CofiRank", "RegSVD", "NMF", "BPMF", "LLORMA", "GCR", "LCR"], "Column_1": ["Metric", "Average Precision", "NDCG@10"], "Column_2": ["N=10", "N=20", "N=50"]}, "indexed_table_260.csv": {"Caption": "Table 5: Robustness of various composite retrieval approaches to initial ranking quality. All systems are measured by nDCG@10. Pairwise t-test significance test is applied, and values marked with Y_(p-value=0.05), IJ(p-value=0.01) and Z_(p-value=0.05), I_(p- value=0.01) indicate respectively significant improvements or deterioration over the original ranking.", "Rows_1": ["FS", "AS", "FSC", "OVS AS", "OVS FS"], "Column_1": ["Original", "BOBO", "BOBO DT", "BOBO DE"]}, "indexed_table_71.csv": {"Caption": "Table 3: Performance of various composite retrieval approaches in the heterogeneous web environment based on FSC (Federated Search Central): central ranking of mixture of all verticals. Pairwise t-test significance test is applied, and values marked with Y_(p- value=0.05), IJ(p-value=0.01) and Z_(p-value=0.05), I_(p-value=0.01) indicate respectively significant improvements or deterioration over baseline (FSC).", "Rows_1": ["P@5", "P@10", "P@30", "nDCG@5", "nDCG@10", "nDCG@30", "tcoh", "tdiv", "vdiv"], "Column_1": ["Composite Retrieval Approaches", "Adding Relevance Estimation"], "Column_2": ["FSC", "BOBO", "BOBO DT BOBO DE CPS", "CPS DT", "CPS DE", "BOBO VT", "BOBO VE", "BOBO E", "BOBO T"]}, "indexed_table_224.csv": {"Caption": "Table 2: Performance Comparison on Epinions and Ciao Dataset", "Rows_1": ["user based CF", "item based CF", "SVD", "PMF", "RSTE", "PLSA MF", "CB Random", "CBMF single", "CBMF"], "Column_2": ["MAP", "Recall@10", "NDCG@10"], "Column_1": ["Epinions", "Ciao"]}, "indexed_table_347.csv": {"Caption": "Table 4: Prediction Results nDCG@10 for check-in and online shopping", "Rows_1": ["Check in", "Shopping"], "Rows_2": ["POI", "Category", "Item"], "Column_1": ["Level", "NSM@Self", "NSM@Crowd", "OF", "MC", "FPMC"]}, "indexed_table_11.csv": {"Caption": "Table 2: The accuracy scores of the four methods. The numbers for each dataset (rows) are the abso- lute estimation error of each method (column). The best performing run in each dataset is highlighted in bold.", "Rows_1": ["ACJ", "SRJ", "CF", "ARJ"], "Column_1": ["MV", "Dawid&Skene", "BCC", "Community BCC"]}, "indexed_table_95.csv": {"Caption": "Table 3: The NLPD scores (the lower the better) of the three probabilistic methods: Dawid&Skene, BCC and CommunityBCC computed from the pre- dictive object class distribution. The best perform- ing run in each dataset is highlighted in bold.", "Rows_1": ["ACJ", "SRJ", "CF", "ARJ"], "Column_1": ["Dawid&Skene", "BCC", "Community BCC"]}, "indexed_table_80.csv": {"Caption": "Table 6: Recommendation performance on the cross-domain collaboration public data [28].", "Rows_1": ["DM to Theory", "MI to DB", "MI to DM", "Visual to DM"], "Rows_2": ["CTL", "SV DNN"], "Column_1": ["P@10", "P@20", "MAP", "R@100"]}, "indexed_table_162.csv": {"Caption": "Table 3: Results of Wikipedia page classification on Wikipedia data set.", "Rows_1": ["Micro-F1", "Macro-F1"], "Rows_2": ["GF", "DeepWalk", "SkipGram", "LINE SGD", "LINE SGD", "LINE", "LINE", "LINE"], "Column_1": ["Training 10%", "Training 20%", "Training 30%", "Training 40%", "Training 50%", "Training 60%", "Training 70%", "Training 80%", "Training 90%"]}, "indexed_table_19.csv": {"Caption": "Table 3: CoNLL03 evaluation of universal training.", "Rows_1": ["LOC", "MISC", "ORG", "PER", "Overall"], "Column_2": ["Prec", "Recall", "F1"], "Column_1": ["Training", "Testing"]}, "indexed_table_38.csv": {"Caption": "Table 4: Comparison with global algorithms on real datasets.", "Rows_1": ["LEMON", "LEMON auto", "DEMON", "OSLOM", "LC"], "Column_1": ["Amazon", "DBLP", "Youtube", "Orkut"], "Column_2": ["F1 score", "time"]}, "indexed_table_380.csv": {"Caption": "Table 2: Comparison among Different Feature Combinations in QuASE", "Rows_1": ["Features in QuASE", "Count", "TR", "JQA", "WAT", "Count+TR", "Count+TR+WAT", "Count+TR+JQA", "Count+TR+WAT+JQA", "Count+TR", "QA Systems", "QuASE", "AskMSR+", "Sempre"], "Column_2": ["Precision", "Recall", "F1", "MRR"], "Column_1": ["Bing query", "TREC"]}, "indexed_table_406.csv": {"Caption": "Table 4: Comparison of Siri, S-Voice, LLM, Word2Vec, SVM and InfoDist on the three domains", "Rows_1": ["Weather", "Maps", "Restaurants"], "Rows_2": ["Accuracy", "Precision", "Recall", "F1 score"], "Column_1": ["Measurements", "Siri", "S Voice", "LLM", "Word2Vec", "SVM linear", "SVM tree", "InfoDist"]}, "indexed_table_7.csv": {"Caption": "Table 2: Precision at top K results for project/product ex- traction", "Rows_1": ["Count", "Capi", "Hybrid", "NB", "SmooNB", "SemiNB", "Our System"], "Column_1": ["P@50", "P@100", "P@200", "P@300", "P@500"]}, "indexed_table_120.csv": {"Caption": "Table 1: Average accuracy (Acc) and coverage (Cov) on relevant hashtags, toponyms and topics, calcu- lated over all tweets (All) and over those labelled as subjective (Subj), positive, (Pos), negative (Neg) and ironic (Iro).", "Rows_1": ["All", "Subj", "Neg", "Pos", "Iro"], "Column_2": ["Acc", "Cov"], "Column_1": ["Hashtags", "Toponyms", "Topics"]}, "indexed_table_390.csv": {"Caption": "Table 2: F-1 Score Comparison", "Rows_1": ["SYNTH 1", "SYNTH 2", "SYNTH 3"], "Column_1": ["VOTE/MEAN", "VOTE/MIN", "VOTE/MAX", "MEAN", "MIN", "MAX", "NMF", "TMSID"]}, "indexed_table_391.csv": {"Caption": "Table 3: Comparison of the best performing ensem- ble model and single-source annotators, listed above. \u201cAll annotator\u201d predicts the union of topic labels from single-source annotators as positive. The best performing algorithm is GBC trained on all five features, having the best overall F1-score here.", "Rows_1": ["Author Name", "Comment", "Photo", "Post Text", "Video", "Web Link", "All Annotator Baseline", "Ensemble Model"], "Column_1": ["F1", "Precision", "Recall"]}, "indexed_table_403.csv": {"Caption": "Table 4: Comparison with WSLOG on set expansion of domain entities.", "Rows_1": ["City", "Country", "Drug", "Food", "Location", "Movie", "Newspaper", "Person", "University", "VideoGame", "Average"], "Column_1": ["WSLOG", "Ours"], "Column_2": ["P", "R", "F1"]}, "indexed_table_139.csv": {"Caption": "Table 1: Accuracy metrics for the full Poisson model. In sample value are trained on and predicted for the same dataset. Out-of-sample are trained on a train set and pre- dicted for a test over 5 fold cross-validation.", "Rows_1": ["pics", "videos", "todayilearned", "news", "worldnews", "Hacker News"], "Column_1": ["R2", "MAE", "MSE", "R2"]}, "indexed_table_211.csv": {"Caption": "Table 5: Classification performance of matrix completion, com- pared to the baselines in terms of precision, recall, F1 score and Matthew\u2019s correlation coefficient. Bold scores are significantly better (based on 99% bootstrapped confidence intervals).", "Rows_1": ["quote popularity", "outlet propensity", "matrix completion"], "Column_1": ["P", "R", "F1", "MCC"]}, "indexed_table_208.csv": {"Caption": "Table 3: Performance of different methods on the three datasets (%)", "Rows_1": ["Weibo", "I2B2", "I2B3", "I2B4", "I2B5", "I2B6", "ICDM'12 Contest", "ICDM'13 Contest", "ICDM'14 Contest", "ICDM'15 Contest", "ICDM'16 Contest"], "Rows_2": ["SM", "RT", "CRF", "CRF+AT", "SOCINST", "SOCINSTbaseRT"], "Column_1": ["Recall", "Precision", "F1 Measure"]}, "indexed_table_291.csv": {"Caption": "Table 1: Movie classification accuracy", "Rows_1": ["LDA", "paragraph2vec", "word2vec", "HDV"], "Column_1": ["drama", "comedy", "thriller", "romance", "action", "crime", "adventure"]}, "indexed_table_396.csv": {"Caption": "None", "Rows_1": ["Facebook", "IMDB", "DVD", "Music", "Comm", "Computers", "Organic"], "Column_1": ["Nv", "Ne", "W", "P"]}, "indexed_table_351.csv": {"Caption": "Table 4: Statistics of daily active sellers, daily new tasks, and the time to undertake a task on the five SRE markets.", "Rows_1": ["Market", "SKY", "WOOD", "EMPIRE", "COOL", "NET"], "Column_2": ["Avg", "Max", "Min"], "Column_1": ["Daily active sellers", "Daily new tasks", "Time to undertake"]}, "indexed_table_135.csv": {"Caption": "Table 7: The best RMSE achieved on four datasets and their averaged value for each method, standard deviations \u2264 0.015 for each experimental run.", "Rows_1": ["NMF", "timeSVD++", "Tensor", "EFM", "FTSA", "FTSA'", "#Users"], "Column_1": ["D1", "D2", "D3", "D4", "Average"]}, "indexed_table_364.csv": {"Caption": "Table 4: Performance of query segmentation on Roy et al. [40] dataset.", "Rows_1": ["N gram IDF", "Mishra et al", "Mishra et al with Wikipedia", "PMI Q", "PMI W", "Unsegmented", "Human A", "Human B", "Human C", "BQV"], "Column_1": ["nDCG@5", "nDCG@10", "MAP@5", "MAP@10", "MRR@5", "MRR@10"]}, "indexed_table_254.csv": {"Caption": "Table 3: Comparative results for hashtag retrieval", "Rows_1": ["Ranking Model", "GR", "RHR", "TFIDFR", "KLDR"], "Column_2": ["MAP", "MRR", "NDCG"], "Column_1": ["Baseline", "Our Approach"]}, "indexed_table_284.csv": {"Caption": "Table 1: Age prediction performance as evaluated through both mean absolute error (MAE) and R2 on Pokec.", "Rows_1": ["Linear Regression + DeepWalk", "Iterative Algorithm", "Neighborhood Average", "Predict Mean"], "Column_2": ["MAE", "R2"], "Column_1": ["5% train", "20% train", "50% train", "80% train", "95% train"]}, "indexed_table_445.csv": {"Caption": "Table 7: Evaluation on top-K ranked answer cells.", "Rows_1": ["Top K", "K=1", "K=2", "K=3", "K=5"], "Column_2": ["Precision", "Recall", "F1"], "Column_1": ["WebQ", "BingQ"]}, "indexed_table_401.csv": {"Caption": "Table 4: Performance of different feature combinations.", "Rows_1": ["Shallow Features", "Deep Features", "Shallow Deep Features", "Shallow DeepType", "Shallow DeepPredicate", "Shallow DeepSentence", "Shallow DeepEntityPairs", "All Features DeepType", "All Features DeepPredicate", "All Features DeepSentence", "All Features DeepEntityPairs"], "Column_1": ["WebQ", "BingQ"], "Column_2": ["Precision", "Recall", "F1"]}, "indexed_table_407.csv": {"Caption": "Table 5: Comparison of different systems.", "Rows_1": ["TabCell", "Sempre", "ParaSempre", "TabCell +ParaSempre"], "Column_1": ["WebQ", "BingQ"], "Column_2": ["Precision", "Recall", "F1"]}, "indexed_table_440.csv": {"Caption": "Table 2: Accuracy of various gender detection methods for people from different countries. For most countries mixed approaches per- form best.", "Rows_1": ["United States", "China", "United Kingdom", "Germany", "Italy", "Canada", "France", "Japan", "Brazil", "Spain", "Australia", "India", "South Korea", "Switzerland Turkey"], "Column_1": ["SSA", "IPUMS", "Sexmachine", "Genderize", "Face++", "Mixed1", "Mixed2"]}, "indexed_table_205.csv": {"Caption": "Table 4: Results on the newer versions of the MSNBC, AQUAINT and ACE04 datasets.", "Rows_1": ["LocalMention", "Cucerzan", "M & W", "Han_11", "AIDA", "GLOW", "RI_13", "REL RW_14", "PBoH"], "Column_2": ["MI-F1", "MA-F1"], "Column_1": ["new MSNBC", "new AQUAINT", "new ACE2004"]}, "indexed_table_379.csv": {"Caption": "Table 5: Phrase relatedness correlation", "Rows_1": ["ESA", "KBLink", "BoW", "ESA C", "LSA", "LDA"], "Column_1": ["Academia", "Yelp"]}, "indexed_table_251.csv": {"Caption": "Table 4: Comparison of Different Recommender Systems in Epinions", "Rows_1": ["Training", "Training50%", "Training70%", "Training90%"], "Rows_2": ["Metric", "MAE", "RMSE"], "Column_1": ["Memory based Methods", "Model based Methods"], "Column_2": ["UCF", "pUCF", "pnUCF", "MF", "SocialMF", "SoReg", "LOCABAL", "disSoReg"]}, "indexed_table_436.csv": {"Caption": "Table 5: Comparison of Different Recommender Systems in Slashdot", "Rows_1": ["P@5", "R@5", "P@10", "R@10"], "Column_1": ["Memory based Methods", "Model based Methods"], "Column_2": ["UCF", "pUCF", "pnUCF", "MF", "SocialMF", "SoReg", "LOCABAL", "disSoReg", "RecSSN"]}, "indexed_table_381.csv": {"Caption": "Table 8: MAP and NDCG@10 of retrieval methods on Robust-04, WT10g, and Gov2 collections where the superscript means significantly higher using a paired t-test (p < 0.05) compared to the standard maximum likelihood query model (e.g. DQMcJM vs JM, and DQMqJM vs JM). _ means significantly higher than TLM, and _ means significant higher compared to BM25 and TLM. Best result in bold.", "Rows_1": ["Retrieval Methods", "MATF", "BM25", "BM25+", "TLM", "JM", "DQMcJM", "DQM qJM", "Dir", "DQMcDir", "DQM 606Nq Dir", "Dir+"], "Column_2": ["MAP", "NDCG"], "Column_1": ["Robust 04 desc", "Robust 04 desc+narr", "WT10g desc", "WT10g desc+narr", "Gov2 desc", "Gov2 desc+narr"]}, "indexed_table_160.csv": {"Caption": "Table 4: Group lifecycle early prediction performance re- sults(%). We train the classifier using all the group-level fea- tures.", "Rows_1": ["1 hour", "1 day", "5 days", "10 days", "20 days", "1 month"], "Column_1": ["AUC", "Prec", "Rec", "F1"]}, "indexed_table_144.csv": {"Caption": "Table 6: Precision and Recall of different models over the label dataset. We set k = 200 to compute the performance of the models.", "Rows_1": ["TwitterRank", "Hashtags", "LDA", "PageRank", "Likes only", "Posts only", "No Wiki", "No weighting", "ALF"], "Column_1": ["Precision", "Recall", "F1 = 2PR"]}, "indexed_table_42.csv": {"Caption": "Table 1: Sentiment analysis of tweets from census tracts. The first three columns give the mean values of valence, arousal and dominance of all tweets from the given set of tracts as measured using Warriner\u2019s lexicon. The last two columns and the mean values of positive and negative sentiment measured by SentiStrength from the same tweets.", "Rows_1": ["Tracts", "All LA Tracts", "Tracts with Check ins", "Tracts with >3 Check ins"], "Column_2": ["# Tract", "Valence", "Arousal", "Dominance", "Positive", "Negative"], "Column_1": ["WKB Lexicon", "SentiStrength"]}}